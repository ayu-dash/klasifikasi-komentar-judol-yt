{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b3b2f4-a622-4456-8bbf-3a5fd75f0b55",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "81151879-d73a-45d6-b0b7-b5ad988a4769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Memuat Data, Augmentasi, dan Preprocessing ---\n",
      "Total data setelah augmentasi dan cleaning: 19444 baris.\n",
      "Jumlah Judol (1) sebelum split: 1633\n",
      "\n",
      "--- 2. Split Data (80% Train, 20% Test Stratified) ---\n",
      "\n",
      "--- 3. Feature Extraction: Tokenisasi dan Padding ---\n",
      "\n",
      "--- 4. Terapkan SMOTE (Oversampling pada Data Latih) ---\n",
      "‚úÖ Data Latih Akhir Setelah SMOTE: 28498 baris (Rasio 1:1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtf/anaconda3/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a27c8983ab544d185516500c869ec38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce6de9d41534d9e9c026176625b67ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memulai Pelatihan Model dengan Data SMOTE (Alpha=0.6)...\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model baru disimpan sebagai judol_detection_augmented_smote.h5 ‚úÖ\n",
      "\n",
      "=============================================\n",
      "üìä EVALUASI AKHIR MODEL PADA DATA UJI MURNI\n",
      "=============================================\n",
      "Loss pada Data Uji: 0.5560\n",
      "Akurasi pada Data Uji: 0.5739\n",
      "\n",
      "Laporan Klasifikasi (Threshold=0.65):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.58      0.74      3562\n",
      "           1       0.18      0.97      0.30       327\n",
      "\n",
      "    accuracy                           0.62      3889\n",
      "   macro avg       0.59      0.78      0.52      3889\n",
      "weighted avg       0.93      0.62      0.70      3889\n",
      "\n",
      "\n",
      "=============================================\n",
      "üöÄ UJI COBA MODEL MURNI ML (SMOTE Trained, Threshold=0.65)\n",
      "=============================================\n",
      "---\n",
      "üí¨ Input: 'WD 15 juta di BOSKU777 langsung cair tanpa ribet!'\n",
      "üßπ Clean: 'wd 15 juta di bosku777 langsung cair tanpa ribet'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.989 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'buruan daftar di MANTAPJEPE888 promonya gede banget'\n",
      "üßπ Clean: 'buruan daftar di mantapjepe888 promonya gede banget'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.987 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'gacor di situs ini, main di kaisar999 depo 50 ribu wd 500 ribu'\n",
      "üßπ Clean: 'gacor di situs ini main di kaisar999 depo 50 ribu wd 500 ribu'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.972 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'main di mahadewa pasti cair'\n",
      "üßπ Clean: 'main di mahadewa pasti cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.961 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'Ini brand baru: GACORWD77. Pasti JP!'\n",
      "üßπ Clean: 'ini brand baru gacorwd77 pasti jp'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.970 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'Coba aja main di brand baru SLOTBARU555, depo langsung wd'\n",
      "üßπ Clean: 'coba aja main di brand baru slotbaru555 depo langsung wd'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.964 | T=0.65)\n",
      "---\n",
      "üí¨ Input: 'nonton film gratis di youtube channel ini aja'\n",
      "üßπ Clean: 'nonton film gratis di youtube channel ini aja'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.253 | T=0.65)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tqdm.keras import TqdmCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n",
    "import unicodedata\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI UTAMA\n",
    "# ===============================================\n",
    "# PASTIKAN FILE PATH SESUAI DENGAN NAMA FILE ANDA\n",
    "FILE_PATH = \"final_production_judol_detection.csv\" \n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "TEST_SIZE_FINAL = 0.20\n",
    "NEW_ALPHA = 0.6 # Alpha untuk Focal Loss, memberi bobot lebih ke kelas Judol (1)\n",
    "OPTIMAL_THRESHOLD = 0.65 \n",
    "TEXT_COLUMN = 'cleaned_comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "# ===============================================\n",
    "# 1. DATA AUGMENTATION: SINTESIS JUDOL BARU\n",
    "# ===============================================\n",
    "\n",
    "# Pola Brand & Kata Kunci\n",
    "prefixes = ['slot', 'jp', 'wd', 'depo', 'gacor', 'raja', 'bosku', 'mantap']\n",
    "numbers = [str(i) for i in range(100, 999, 100)] + [str(i) for i in range(11, 99, 11)]\n",
    "phrases = ['gacor banget di sini!', 'pasti jp di situs ini', 'wd cepat tanpa ribet', 'cair terus bosku']\n",
    "\n",
    "synthetic_data = []\n",
    "for i in range(200): \n",
    "    prefix = np.random.choice(prefixes)\n",
    "    num = np.random.choice(numbers)\n",
    "    phrase = np.random.choice(phrases)\n",
    "    \n",
    "    brand_name = prefix + num\n",
    "    comment = f\"{phrase} coba main di {brand_name} aja deh\"\n",
    "    \n",
    "    # Cleaning dasar untuk data sintetis\n",
    "    cleaned_comment = re.sub(r'\\s+', ' ', re.sub(r\"[^a-z0-9\\s]\", \" \", comment.lower())).strip()\n",
    "    \n",
    "    synthetic_data.append({\n",
    "        \"comment_text\": comment,\n",
    "        \"target\": 1,\n",
    "        \"cleaned_comment_text\": cleaned_comment,\n",
    "        \"combined_text\": cleaned_comment + \" \" + comment \n",
    "    })\n",
    "\n",
    "df_synthetic = pd.DataFrame(synthetic_data)\n",
    "df_synthetic['cleaned_comment_text'] = df_synthetic['cleaned_comment_text'].str.replace('coba main di ', '').str.replace(' aja deh', '')\n",
    "\n",
    "# ===============================================\n",
    "# 2. LOAD & GABUNGKAN DATA\n",
    "# ===============================================\n",
    "\n",
    "def merge_spaced_characters(text):\n",
    "    \"\"\"\n",
    "    Menggabungkan pola huruf/angka yang dipisah spasi namun merupakan satu kata.\n",
    "    Contoh: 'p u l a u w i n 8 8' -> 'pulauwin88'\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    merged_tokens = []\n",
    "    buffer = []\n",
    "\n",
    "    def flush_buffer():\n",
    "        if len(buffer) >= 3:  # Minimal 3 karakter untuk dianggap kata terpisah\n",
    "            merged_tokens.append(\"\".join(buffer))\n",
    "        else:\n",
    "            merged_tokens.extend(buffer)\n",
    "        buffer.clear()\n",
    "\n",
    "    for tok in tokens:\n",
    "        if len(tok) == 1 and re.match(r\"[a-zA-Z0-9]\", tok):\n",
    "            buffer.append(tok.lower())\n",
    "        else:\n",
    "            flush_buffer()\n",
    "            merged_tokens.append(tok)\n",
    "\n",
    "    flush_buffer()\n",
    "    return \" \".join(merged_tokens)\n",
    "\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning yang digunakan untuk Tokenisasi.\"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    text = merge_spaced_characters(text)\n",
    "    \n",
    "    # NFKD: Mengubah karakter fancy (misal ùóõùó¢ùóßùêâùêÑùêèùêÑùü±ùü±) menjadi bentuk dasarnya (HOTJEPE55)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Encode/Decode: Menghapus semua karakter non-ASCII (seperti emoji üÄÑü•∞, diakritik)\n",
    "    # yang tersisa setelah NFKD, hanya menyisakan karakter dasar ASCII.\n",
    "    # Ini adalah langkah kunci untuk mengatasi masalah hilangnya karakter fancy.\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 3. Lowercase (Dapat dilakukan setelah konversi ASCII)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 4. Hapus sisa karakter yang bukan a-z, 0-9, atau spasi\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # 5. Rapikan spasi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"--- 1. Memuat Data, Augmentasi, dan Preprocessing ---\")\n",
    "try:\n",
    "    df_original = pd.read_csv(FILE_PATH)\n",
    "    # Pastikan kolom utama ada dan target numerik\n",
    "    df_original = df_original.dropna(subset=[TEXT_COLUMN, TARGET_COLUMN])\n",
    "    df_original[TARGET_COLUMN] = df_original[TARGET_COLUMN].astype(int)\n",
    "    \n",
    "    # Gabungkan data asli dengan data sintetik\n",
    "    df_combined = pd.concat([df_original[['comment_text', 'cleaned_comment_text', 'target', 'combined_text']], df_synthetic], ignore_index=True)\n",
    "\n",
    "    # Bersihkan Teks pada kolom fitur utama\n",
    "    df_combined[TEXT_COLUMN] = df_combined[TEXT_COLUMN].fillna('').apply(clean_text_prep)\n",
    "    \n",
    "    # Filter data yang teksnya tidak kosong\n",
    "    df_combined = df_combined[df_combined[TEXT_COLUMN] != '']\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File '{FILE_PATH}' tidak ditemukan.\")\n",
    "    exit()\n",
    "\n",
    "X = df_combined[TEXT_COLUMN]\n",
    "Y = df_combined[TARGET_COLUMN]\n",
    "\n",
    "print(f\"Total data setelah augmentasi dan cleaning: {len(X)} baris.\")\n",
    "print(f\"Jumlah Judol (1) sebelum split: {Y.sum()}\")\n",
    "\n",
    "# ===============================================\n",
    "# 3. SPLIT DATA (STRATIFIED)\n",
    "# ===============================================\n",
    "print(\"\\n--- 2. Split Data (80% Train, 20% Test Stratified) ---\")\n",
    "# Pembagian 80% Train, 20% Test Murni\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    X, \n",
    "    Y, \n",
    "    test_size=TEST_SIZE_FINAL, \n",
    "    random_state=SEED, \n",
    "    stratify=Y \n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# 4. FEATURE EXTRACTION (Tokenisasi & Padding)\n",
    "# ===============================================\n",
    "print(\"\\n--- 3. Feature Extraction: Tokenisasi dan Padding ---\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text.astype(str))\n",
    "\n",
    "# SIMPAN TOKENIZER\n",
    "with open('tokenizer_augmented.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Ubah Data Latih menjadi Vektor Fitur\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text.astype(str))\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Ubah Data Uji (Murni) menjadi Vektor Fitur\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text.astype(str))\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 5. TERAPKAN SMOTE (HANYA PADA DATA LATIH)\n",
    "# ===============================================\n",
    "print(\"\\n--- 4. Terapkan SMOTE (Oversampling pada Data Latih) ---\")\n",
    "Y_train_np = Y_train.values \n",
    "smote = SMOTE(sampling_strategy='minority', random_state=SEED)\n",
    "\n",
    "# SMOTE diterapkan pada fitur numerik (X_train_padded) dan labelnya (Y_train_np)\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_train_padded, Y_train_np)\n",
    "\n",
    "# Hasil Akhir (Data Latih yang Sudah Seimbang)\n",
    "X_train_final = np.array(X_resampled)\n",
    "Y_train_final = np.array(Y_resampled)\n",
    "\n",
    "# Verifikasi Rasio Kelas\n",
    "judol_count = Y_train_final.sum()\n",
    "non_judol_count = len(Y_train_final) - judol_count\n",
    "print(f\"‚úÖ Data Latih Akhir Setelah SMOTE: {len(X_train_final)} baris (Rasio 1:{round(non_judol_count / judol_count)})\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 6. TRAINING DENGAN DATA SMOTE\n",
    "# ===============================================\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                 - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "    \n",
    "model_aug = Sequential([\n",
    "    Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model_aug.compile(loss=focal_loss(), optimizer=Adam(learning_rate=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "    TqdmCallback(verbose=1),\n",
    "]\n",
    "\n",
    "print(f\"\\nMemulai Pelatihan Model dengan Data SMOTE (Alpha={NEW_ALPHA})...\")\n",
    "\n",
    "history = model_aug.fit(\n",
    "    X_train_final, Y_train_final, # <-- MENGGUNAKAN DATA HASIL SMOTE\n",
    "    validation_split=0.1, # Menggunakan 10% dari data SMOTE untuk validasi\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # class_weight TIDAK DIPERLUKAN KARENA DATA SUDAH SEIMBANG OLEH SMOTE\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# SAVE MODEL BARU\n",
    "NEW_MODEL_AUG_PATH = \"judol_detection_augmented_smote.h5\"\n",
    "model_aug.save(NEW_MODEL_AUG_PATH)\n",
    "print(f\"\\nModel baru disimpan sebagai {NEW_MODEL_AUG_PATH} ‚úÖ\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 7. EVALUASI DAN UJI MODEL BARU (KODE PERBAIKAN)\n",
    "# ===============================================\n",
    "\n",
    "print(\"\\n=============================================\")\n",
    "print(\"üìä EVALUASI AKHIR MODEL PADA DATA UJI MURNI\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "# FIX: Konversi Y_test (Pandas Series) menjadi NumPy Array untuk menghindari ValueError\n",
    "Y_test_np = Y_test.values \n",
    "\n",
    "# Evaluasi Model pada Data Uji Murni (X_test_padded)\n",
    "loss, accuracy = model_aug.evaluate(X_test_padded, Y_test_np, verbose=0) \n",
    "\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")\n",
    "print(f\"Akurasi pada Data Uji: {accuracy:.4f}\")\n",
    "\n",
    "# Hitung Precision, Recall, dan F1-Score menggunakan OPTIMAL_THRESHOLD\n",
    "y_pred_proba = model_aug.predict(X_test_padded, verbose=0)\n",
    "y_pred_classes = (y_pred_proba > OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\nLaporan Klasifikasi (Threshold={OPTIMAL_THRESHOLD}):\")\n",
    "print(classification_report(Y_test_np, y_pred_classes)) # Menggunakan Y_test_np\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 8. UJI MANUAL PADA KASUS SULIT (Pure ML)\n",
    "# ===============================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Fungsi cleaning yang digunakan saat prediksi (dengan normalisasi Unicode)\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Normalisasi Unicode\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') \n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def predict_text_ml_only(text, model, tokenizer, threshold=OPTIMAL_THRESHOLD):\n",
    "    \"\"\"Melakukan prediksi Judol murni tanpa Heuristik.\"\"\"\n",
    "    \n",
    "    text_clean = clean_text(text)\n",
    "    \n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    pred_prob = model.predict(pad, verbose=0)[0][0]\n",
    "    is_judol = pred_prob > threshold\n",
    "    \n",
    "    label = \"üî• **JUDOL (SMOTE ML)**\" if is_judol else \"‚úÖ Bukan Judol (SMOTE ML)\"\n",
    "    \n",
    "    print(f\"---\")\n",
    "    print(f\"üí¨ Input: '{text}'\")\n",
    "    print(f\"üßπ Clean: '{text_clean}'\")\n",
    "    print(f\"üéØ Prediksi: {label} (Probabilitas: {pred_prob:.3f} | T={threshold})\")\n",
    "\n",
    "manual_data = [\n",
    "    \"WD 15 juta di BOSKU777 langsung cair tanpa ribet!\", \n",
    "    \"buruan daftar di MANTAPJEPE888 promonya gede banget\", \n",
    "    \"gacor di situs ini, main di kaisar999 depo 50 ribu wd 500 ribu\", \n",
    "    \"main di mahadewa pasti cair\",\n",
    "    \"Ini brand baru: GACORWD77. Pasti JP!\", \n",
    "    \"Coba aja main di brand baru SLOTBARU555, depo langsung wd\", \n",
    "    \"nonton film gratis di youtube channel ini aja\",\n",
    "]\n",
    "\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"üöÄ UJI COBA MODEL MURNI ML (SMOTE Trained, Threshold={OPTIMAL_THRESHOLD})\")\n",
    "print(\"=============================================\")\n",
    "for data in manual_data:\n",
    "    predict_text_ml_only(data, model_aug, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "421732b4-552c-4810-bbc5-a32c4ad3aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading data and preparing robust augmentations ---\n",
      "Total rows after augmentation & cleaning: 21773\n",
      "Label distribution (overall):\n",
      "target\n",
      "0    18858\n",
      "1     2915\n",
      "Name: count, dtype: int64\n",
      "Tokenizer saved.\n",
      "Applying SMOTE to training data...\n",
      "After SMOTE: 30172 samples; Positives: 15086\n",
      "Start training...\n",
      "Epoch 1/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 329ms/step - accuracy: 0.8668 - loss: 0.0396 - val_accuracy: 0.8376 - val_loss: 0.0480 - learning_rate: 0.0010\n",
      "Epoch 2/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 314ms/step - accuracy: 0.9509 - loss: 0.0171 - val_accuracy: 0.9102 - val_loss: 0.0321 - learning_rate: 0.0010\n",
      "Epoch 3/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 315ms/step - accuracy: 0.9785 - loss: 0.0076 - val_accuracy: 0.8694 - val_loss: 0.0800 - learning_rate: 0.0010\n",
      "Epoch 4/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.9884 - loss: 0.0046  \n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 319ms/step - accuracy: 0.9892 - loss: 0.0039 - val_accuracy: 0.8883 - val_loss: 0.0751 - learning_rate: 0.0010\n",
      "Epoch 5/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 321ms/step - accuracy: 0.9943 - loss: 0.0022 - val_accuracy: 0.8840 - val_loss: 0.0952 - learning_rate: 5.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.9960 - loss: 0.0016  \n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m425/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 313ms/step - accuracy: 0.9959 - loss: 0.0015 - val_accuracy: 0.8996 - val_loss: 0.0845 - learning_rate: 5.0000e-04\n",
      "Saving model...\n",
      "Model saved to: judol_detection_augmented_smote_robust.keras\n",
      "Evaluating on test set (pure real-world test set)...\n",
      "Test Loss: 0.5197  Test Acc: 0.5743\n",
      "Classification report (threshold 0.65):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.54      0.70      3772\n",
      "           1       0.24      0.96      0.39       583\n",
      "\n",
      "    accuracy                           0.59      4355\n",
      "   macro avg       0.62      0.75      0.54      4355\n",
      "weighted avg       0.89      0.59      0.65      4355\n",
      "\n",
      "\n",
      "Quick manual checks:\n",
      "{'input': 'W D 15 juta di B O S K U 7 7 7 langsung cair!', 'clean': 'w d 15 juta di bosku777 langsung cair', 'prob': 0.9888522028923035, 'label': 1}\n",
      "{'input': 'p.u.l.a.u.w.i.n.8.8 jp wd cepat', 'clean': 'p u l a u w i n 8 8 jp wd cepat', 'prob': 0.9590727090835571, 'label': 1}\n",
      "{'input': 'mantapjepe888 yuk main sini', 'clean': 'mantapjepe888 yuk main sini', 'prob': 0.9916000366210938, 'label': 1}\n",
      "{'input': 'slotbaru555 gacor', 'clean': 'slotbaru555 gacor', 'prob': 0.9922066330909729, 'label': 1}\n",
      "{'input': 'nonton film gratis di youtube channel ini saja', 'clean': 'nonton film gratis di youtube channel ini saja', 'prob': 0.5434849858283997, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# file: robust_judol_training_and_prediction.py\n",
    "# Versi: Robust augmentation + unified preprocessing + training + evaluation + prediction util\n",
    "# Tujuan: model lebih kebal terhadap pola obfuscation yang tidak ada di dataset.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FILE_PATH = \"final_production_judol_detection.csv\"\n",
    "MAX_WORDS = 30000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "TEST_SIZE_FINAL = 0.20\n",
    "NEW_ALPHA = 0.55\n",
    "OPTIMAL_THRESHOLD = 0.65\n",
    "TEXT_COLUMN = 'combined_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "# Paths to save\n",
    "TOKENIZER_PATH = \"tokenizer_augmented_robust.pickle\"\n",
    "MODEL_PATH = \"judol_detection_augmented_smote_robust.keras\"  # prefer native Keras format\n",
    "\n",
    "# =========================\n",
    "# UTIL: zero-width remover, fancy-char normalizer, etc.\n",
    "# =========================\n",
    "ZERO_WIDTH_CHARS = [\n",
    "    '\\u200b', '\\u200c', '\\u200d', '\\u2060', '\\uFEFF'\n",
    "]\n",
    "\n",
    "def remove_zero_width(text: str) -> str:\n",
    "    for z in ZERO_WIDTH_CHARS:\n",
    "        text = text.replace(z, '')\n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    # Turn fancy fonts into decomposed forms, then drop non-ascii\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# =========================\n",
    "# STRONG MERGE: merge sequences of single-chars separated by spaces/dots/underscores/other separators\n",
    "# - handles \"p u l a u w i n 8 8\", \"p.u.l.a.u.w.i.n\", \"p_u_l_a_u\", \"p u l a u ‚Ä¢ w i n\"\n",
    "# - only merges sequences >= min_len (default 3) to avoid accidental merges\n",
    "# =========================\n",
    "SEP_CHARS = r\"[\\s\\.\\-_\\|‚Ä¢¬∑\\u2063]\"  # add common separators (space,dot, dash, underscore, pipe, bullet, invisible joiner)\n",
    "MIN_MERGE_LEN = 3\n",
    "\n",
    "def merge_obfuscated_sequences(text: str, min_len=MIN_MERGE_LEN) -> str:\n",
    "    # Replace runs like: a . b . c . 1 2 -> abc12\n",
    "    # Strategy: find sequences of single-letter/digit tokens separated by sep chars and merge them\n",
    "    # We'll token-scan (character-level) but mindful of real words\n",
    "    tokens = re.split(r'(\\s+)', text)  # keep whitespace tokens to preserve boundaries\n",
    "    out_tokens = []\n",
    "    buffer = []\n",
    "    for tok in tokens:\n",
    "        # If tok is whitespace, flush and keep it\n",
    "        if tok.isspace():\n",
    "            if buffer:\n",
    "                # decide to merge or keep\n",
    "                if len(buffer) >= min_len and all(re.fullmatch(r'[A-Za-z0-9]', c) or re.fullmatch(SEP_CHARS, c) for c in \"\".join(buffer)):\n",
    "                    # normalize: remove separators, join alnum\n",
    "                    merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "                    out_tokens.append(merged)\n",
    "                else:\n",
    "                    out_tokens.append(\"\".join(buffer))\n",
    "                buffer = []\n",
    "            out_tokens.append(tok)\n",
    "            continue\n",
    "\n",
    "        # If token is composed of single letters/digits separated by sep inside (like 'p.u.l.a'), detect:\n",
    "        inner_chars = list(tok)\n",
    "        # Heuristic: if token length small-ish and contains separators or only single chars separated,\n",
    "        # add to buffer as sequence chunk\n",
    "        if re.fullmatch(r'(?:[A-Za-z0-9]|' + SEP_CHARS + r')+$', tok):\n",
    "            # put token into buffer char by char\n",
    "            buffer.append(tok)\n",
    "        else:\n",
    "            # flush buffer\n",
    "            if buffer:\n",
    "                if len(buffer) >= min_len:\n",
    "                    merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "                    out_tokens.append(merged)\n",
    "                else:\n",
    "                    out_tokens.append(\"\".join(buffer))\n",
    "                buffer = []\n",
    "            out_tokens.append(tok)\n",
    "    # final flush\n",
    "    if buffer:\n",
    "        if len(buffer) >= min_len:\n",
    "            merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "            out_tokens.append(merged)\n",
    "        else:\n",
    "            out_tokens.append(\"\".join(buffer))\n",
    "\n",
    "    return \"\".join(out_tokens)\n",
    "\n",
    "# Alternative simpler merge for 'space-separated tokens' e.g. \"p u l a u\"\n",
    "def merge_spaced_characters_simple(text: str, min_len=MIN_MERGE_LEN) -> str:\n",
    "    tokens = text.split()\n",
    "    merged = []\n",
    "    buffer = []\n",
    "    for t in tokens:\n",
    "        if len(t) == 1 and re.match(r'[A-Za-z0-9]', t):\n",
    "            buffer.append(t)\n",
    "        else:\n",
    "            if len(buffer) >= min_len:\n",
    "                merged.append(\"\".join(buffer))\n",
    "            elif buffer:\n",
    "                merged.extend(buffer)\n",
    "            buffer = []\n",
    "            merged.append(t)\n",
    "    if buffer:\n",
    "        if len(buffer) >= min_len:\n",
    "            merged.append(\"\".join(buffer))\n",
    "        else:\n",
    "            merged.extend(buffer)\n",
    "    return \" \".join(merged)\n",
    "\n",
    "# We'll apply both merge strategies (simple spaced + sep-aware) in pipeline.\n",
    "\n",
    "# =========================\n",
    "# AUGMENTATION HELPERS: create obfuscated variants for a base brand/text\n",
    "# - spaced, dotted, underscored, insert zero-width, leet sub, random splits\n",
    "# =========================\n",
    "LEET_MAP = str.maketrans({\n",
    "    'a': '4', 'o': '0', 'i': '1', 'e': '3', 's': '5', 't': '7', 'b': '8', 'g': '9'\n",
    "})\n",
    "\n",
    "def to_leet(s: str):\n",
    "    return s.translate(LEET_MAP)\n",
    "\n",
    "def spaced_variant(s: str):\n",
    "    return \" \".join(list(s))\n",
    "\n",
    "def dotted_variant(s: str):\n",
    "    return \".\".join(list(s))\n",
    "\n",
    "def underscored_variant(s: str):\n",
    "    return \"_\".join(list(s))\n",
    "\n",
    "def zero_width_variant(s: str):\n",
    "    # insert zero-width joiner at random positions\n",
    "    zw = '\\u200b'\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch)\n",
    "        if random.random() < 0.25:\n",
    "            out.append(zw)\n",
    "    return \"\".join(out)\n",
    "\n",
    "def random_split_variant(s: str):\n",
    "    # randomly insert spaces inside the word (simulate weird splits)\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch)\n",
    "        if random.random() < 0.18:\n",
    "            out.append(\" \")\n",
    "    return \"\".join(out)\n",
    "\n",
    "def punctuated_variant(s: str):\n",
    "    # insert random separators from a set\n",
    "    seps = ['.', '-', '_', '¬∑', '‚Ä¢', '|']\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch)\n",
    "        if random.random() < 0.18:\n",
    "            out.append(random.choice(seps))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def generate_obfuscations(s: str, n=6):\n",
    "    variants = set()\n",
    "    variants.add(s)\n",
    "    variants.add(s.lower())\n",
    "    variants.add(to_leet(s.lower()))\n",
    "    variants.add(spaced_variant(s.lower()))\n",
    "    variants.add(dotted_variant(s.lower()))\n",
    "    variants.add(underscored_variant(s.lower()))\n",
    "    variants.add(zero_width_variant(s.lower()))\n",
    "    variants.add(random_split_variant(s.lower()))\n",
    "    variants.add(punctuated_variant(s.lower()))\n",
    "    # trim and unique\n",
    "    variants = list(v for v in variants if v and len(v) > 0)\n",
    "    random.shuffle(variants)\n",
    "    return variants[:n]\n",
    "\n",
    "# =========================\n",
    "# CLEANING PIPELINE (UNIFIED FOR TRAIN & PREDICT)\n",
    "# Steps:\n",
    "# 1) remove URLs\n",
    "# 2) remove zero-width\n",
    "# 3) normalize unicode -> ascii\n",
    "# 4) apply merge_spaced_characters_simple (space-only)\n",
    "# 5) apply merge_obfuscated_sequences (sep-aware)\n",
    "# 6) lowercase, remove remaining non-alnum\n",
    "# 7) collapse spaces\n",
    "# =========================\n",
    "def clean_text_unified(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.strip()\n",
    "    # remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    # remove zero-width\n",
    "    text = remove_zero_width(text)\n",
    "    # normalize fancy unicode to ascii\n",
    "    text = normalize_unicode(text)\n",
    "    # apply basic collapse of weird repeated punctuation\n",
    "    text = re.sub(r'[\\u2000-\\u206F\\u2E00-\\u2E7F\\p{P}]+', ' ', text) if False else text\n",
    "    # first pass: merge purely spaced tokens\n",
    "    text = merge_spaced_characters_simple(text)\n",
    "    # second pass: merge tokens separated by punctuation/dots/underscores\n",
    "    text = merge_obfuscated_sequences(text)\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # replace non alphanumeric with space\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # collapse spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================\n",
    "# LOAD, AUGMENT, CLEAN\n",
    "# =========================\n",
    "print(\"--- Loading data and preparing robust augmentations ---\")\n",
    "try:\n",
    "    df_original = pd.read_csv(FILE_PATH)\n",
    "    df_original = df_original.dropna(subset=[TEXT_COLUMN, TARGET_COLUMN])\n",
    "    df_original[TARGET_COLUMN] = df_original[TARGET_COLUMN].astype(int)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: '{FILE_PATH}' not found. Aborting.\")\n",
    "    raise\n",
    "\n",
    "# build synthetic base brands and phrases (wider variety)\n",
    "prefixes = ['slot', 'jp', 'wd', 'depo', 'gacor', 'raja', 'bosku', 'mantap', 'pulau', 'vip', 'maxwin', 'king', 'boss']\n",
    "numbers = [str(i) for i in range(1, 1000, 11)]\n",
    "phrases = [\n",
    "    'pasti jp di sini', 'wd cepat cair', 'deposito langsung', 'gacor banget', \n",
    "    'langsung wd tanpa ribet', 'buruan daftar', 'bonus untuk depo'\n",
    "]\n",
    "\n",
    "synthetic_rows = []\n",
    "random.seed(SEED)\n",
    "# create many synthetic base strings and many obfuscated variants\n",
    "for i in range(200):  # more synthetic examples for robustness\n",
    "    prefix = random.choice(prefixes)\n",
    "    num = random.choice(numbers)\n",
    "    brand = f\"{prefix}{num}\"\n",
    "    phrase = random.choice(phrases)\n",
    "    base_text = f\"{phrase} main di {brand}\"\n",
    "    # create multiple obfuscated variants per base\n",
    "    variants = generate_obfuscations(brand, n=8)\n",
    "    for v in variants:\n",
    "        # combine with phrase and sometimes with different markers\n",
    "        form = random.choice([\n",
    "            f\"{phrase} {v}\",\n",
    "            f\"buruan {v} jp\",\n",
    "            f\"wd cepat di {v}\",\n",
    "            f\"main di {v} aja\",\n",
    "            f\"{v} gacor jp wd\"\n",
    "        ])\n",
    "        cleaned = clean_text_unified(form)\n",
    "        synthetic_rows.append({\n",
    "            \"comment_text\": form,\n",
    "            \"cleaned_comment_text\": cleaned,\n",
    "            \"target\": 1,\n",
    "            TEXT_COLUMN: cleaned\n",
    "        })\n",
    "\n",
    "df_synth = pd.DataFrame(synthetic_rows)\n",
    "\n",
    "# combine with original but keep original cleaned if exists; otherwise clean\n",
    "if 'cleaned_comment_text' not in df_original.columns:\n",
    "    df_original['cleaned_comment_text'] = df_original[TEXT_COLUMN].astype(str).apply(clean_text_unified)\n",
    "else:\n",
    "    df_original[TEXT_COLUMN] = df_original[TEXT_COLUMN].astype(str).apply(clean_text_unified)\n",
    "    df_original['cleaned_comment_text'] = df_original['cleaned_comment_text'].astype(str).apply(clean_text_unified)\n",
    "\n",
    "# Combine\n",
    "df_combined = pd.concat([df_original[[ 'comment_text', 'cleaned_comment_text', TARGET_COLUMN, TEXT_COLUMN ]], df_synth], ignore_index=True)\n",
    "df_combined[TEXT_COLUMN] = df_combined[TEXT_COLUMN].fillna('').astype(str).apply(clean_text_unified)\n",
    "\n",
    "# filter empties\n",
    "df_combined = df_combined[df_combined[TEXT_COLUMN] != '']\n",
    "\n",
    "print(f\"Total rows after augmentation & cleaning: {len(df_combined)}\")\n",
    "print(\"Label distribution (overall):\")\n",
    "print(df_combined[TARGET_COLUMN].value_counts())\n",
    "\n",
    "# =========================\n",
    "# TRAIN-TEST SPLIT\n",
    "# =========================\n",
    "X = df_combined[TEXT_COLUMN]\n",
    "Y = df_combined[TARGET_COLUMN].astype(int)\n",
    "\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=TEST_SIZE_FINAL, random_state=SEED, stratify=Y\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# TOKENIZER\n",
    "# =========================\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text.astype(str))\n",
    "with open(TOKENIZER_PATH, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Tokenizer saved.\")\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text.astype(str))\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text.astype(str))\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# =========================\n",
    "# SMOTE (on padded sequences)\n",
    "# =========================\n",
    "print(\"Applying SMOTE to training data...\")\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=SEED)\n",
    "Y_train_np = Y_train.values\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_train_padded, Y_train_np)\n",
    "X_train_final = np.array(X_resampled)\n",
    "Y_train_final = np.array(Y_resampled)\n",
    "print(f\"After SMOTE: {len(X_train_final)} samples; Positives: {Y_train_final.sum()}\")\n",
    "\n",
    "# =========================\n",
    "# MODEL: Bidirectional LSTM + Pooling\n",
    "# =========================\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss_value = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss_value)\n",
    "    return loss\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=MAX_WORDS, output_dim=EMBED_DIM),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=focal_loss(), optimizer=Adam(learning_rate=LR), metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Start training...\")\n",
    "history = model.fit(\n",
    "    X_train_final, Y_train_final,\n",
    "    validation_split=0.1,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"Model saved to: {MODEL_PATH}\")\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "print(\"Evaluating on test set (pure real-world test set)...\")\n",
    "Y_test_np = Y_test.values\n",
    "loss, acc = model.evaluate(X_test_padded, Y_test_np, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}  Test Acc: {acc:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(X_test_padded, verbose=0)\n",
    "y_pred = (y_pred_proba > OPTIMAL_THRESHOLD).astype(int)\n",
    "print(\"Classification report (threshold {:.2f}):\".format(OPTIMAL_THRESHOLD))\n",
    "print(classification_report(Y_test_np, y_pred))\n",
    "\n",
    "# =========================\n",
    "# PREDICTION FUNCTIONS (use same cleaning + tokenizer + model)\n",
    "# =========================\n",
    "def load_artifacts(model_path=MODEL_PATH, tokenizer_path=TOKENIZER_PATH):\n",
    "    tk = None\n",
    "    mdl = None\n",
    "    if os.path.exists(tokenizer_path):\n",
    "        with open(tokenizer_path, 'rb') as f:\n",
    "            tk = pickle.load(f)\n",
    "    if os.path.exists(model_path):\n",
    "        mdl = load_model(model_path, custom_objects={'loss': focal_loss(alpha=NEW_ALPHA)})\n",
    "    return mdl, tk\n",
    "\n",
    "def predict_single(text: str, model_obj, tokenizer_obj, threshold=OPTIMAL_THRESHOLD):\n",
    "    text_clean = clean_text_unified(text)\n",
    "    seq = tokenizer_obj.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    prob = model_obj.predict(pad, verbose=0)[0][0]\n",
    "    label = 1 if prob > threshold else 0\n",
    "    return {\n",
    "        \"input\": text,\n",
    "        \"clean\": text_clean,\n",
    "        \"prob\": float(prob),\n",
    "        \"label\": int(label)\n",
    "    }\n",
    "\n",
    "# Example quick test after training:\n",
    "print(\"\\nQuick manual checks:\")\n",
    "examples = [\n",
    "    \"W D 15 juta di B O S K U 7 7 7 langsung cair!\",\n",
    "    \"p.u.l.a.u.w.i.n.8.8 jp wd cepat\",\n",
    "    \"mantapjepe888 yuk main sini\",\n",
    "    \"slotbaru555 gacor\",\n",
    "    \"nonton film gratis di youtube channel ini saja\"\n",
    "]\n",
    "mdl, tk = load_artifacts()\n",
    "if mdl and tk:\n",
    "    for ex in examples:\n",
    "        r = predict_single(ex, mdl, tk)\n",
    "        print(r)\n",
    "else:\n",
    "    print(\"Artifacts not found to run quick predict demo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bbc53a-d82a-46ab-bc67-dc5fabd8d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FILE: test_judol_model.py\n",
    "# Standalone tester for robust judol model\n",
    "# ============================================\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ============================================\n",
    "# CONFIG\n",
    "# ============================================\n",
    "MODEL_PATH = \"judol_detection_augmented_smote_robust.keras\"\n",
    "TOKENIZER_PATH = \"tokenizer_augmented_robust.pickle\"\n",
    "MAX_LEN = 100\n",
    "OPTIMAL_THRESHOLD = 0.65\n",
    "\n",
    "# ============================================\n",
    "# CLEANING PIPELINE (copy dari training file)\n",
    "# ============================================\n",
    "\n",
    "ZERO_WIDTH_CHARS = ['\\u200b', '\\u200c', '\\u200d', '\\u2060', '\\uFEFF']\n",
    "SEP_CHARS = r\"[\\s\\.\\-_\\|‚Ä¢¬∑\\u2063]\"\n",
    "MIN_MERGE_LEN = 3\n",
    "\n",
    "def remove_zero_width(text):\n",
    "    for z in ZERO_WIDTH_CHARS:\n",
    "        text = text.replace(z, '')\n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text):\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "    return text\n",
    "\n",
    "def merge_spaced_characters_simple(text, min_len=MIN_MERGE_LEN):\n",
    "    tokens = text.split()\n",
    "    merged, buffer = [], []\n",
    "    for t in tokens:\n",
    "        if len(t) == 1 and re.match(r'[A-Za-z0-9]', t):\n",
    "            buffer.append(t)\n",
    "        else:\n",
    "            if len(buffer) >= min_len:\n",
    "                merged.append(\"\".join(buffer))\n",
    "            elif buffer:\n",
    "                merged.extend(buffer)\n",
    "            buffer = []\n",
    "            merged.append(t)\n",
    "    if buffer:\n",
    "        if len(buffer) >= min_len:\n",
    "            merged.append(\"\".join(buffer))\n",
    "        else:\n",
    "            merged.extend(buffer)\n",
    "    return \" \".join(merged)\n",
    "\n",
    "def merge_obfuscated_sequences(text, min_len=MIN_MERGE_LEN):\n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "    out, buffer = [], []\n",
    "    for tok in tokens:\n",
    "        if tok.isspace():\n",
    "            if buffer:\n",
    "                if len(buffer) >= min_len:\n",
    "                    merged = re.sub(SEP_CHARS, \"\", \"\".join(buffer))\n",
    "                    out.append(merged)\n",
    "                else:\n",
    "                    out.append(\"\".join(buffer))\n",
    "                buffer = []\n",
    "            out.append(tok)\n",
    "            continue\n",
    "        if re.fullmatch(r'(?:[A-Za-z0-9]|' + SEP_CHARS + r')+$', tok):\n",
    "            buffer.append(tok)\n",
    "        else:\n",
    "            if buffer:\n",
    "                if len(buffer) >= min_len:\n",
    "                    merged = re.sub(SEP_CHARS, \"\", \"\".join(buffer))\n",
    "                    out.append(merged)\n",
    "                else:\n",
    "                    out.append(\"\".join(buffer))\n",
    "                buffer = []\n",
    "            out.append(tok)\n",
    "    if buffer:\n",
    "        if len(buffer) >= min_len:\n",
    "            out.append(re.sub(SEP_CHARS, \"\", \"\".join(buffer)))\n",
    "        else:\n",
    "            out.append(\"\".join(buffer))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def clean_text_unified(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = remove_zero_width(text)\n",
    "    text = normalize_unicode(text)\n",
    "\n",
    "    text = merge_spaced_characters_simple(text)\n",
    "    text = merge_obfuscated_sequences(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# ============================================\n",
    "# LOAD MODEL + TOKENIZER\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = load_model(MODEL_PATH)\n",
    "\n",
    "print(\"Artifacts loaded!\")\n",
    "\n",
    "# ============================================\n",
    "# PREDICT FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def predict(text):\n",
    "    clean = clean_text_unified(text)\n",
    "    seq = tokenizer.texts_to_sequences([clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "\n",
    "    prob = float(model.predict(pad, verbose=0)[0][0])\n",
    "    label = 1 if prob >= OPTIMAL_THRESHOLD else 0\n",
    "\n",
    "    return {\n",
    "        \"input\": text,\n",
    "        \"clean\": clean,\n",
    "        \"prob\": prob,\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# TEST EXAMPLES\n",
    "# ============================================\n",
    "\n",
    "tests = [\n",
    "    \"W D 15 juta di B O S K U 7 7 7 langsung cair!\",\n",
    "    \"p.u.l.a.u.w.i.n.8.8 jp wd cepat\",\n",
    "    \"mantapjepe888 yuk main sini\",\n",
    "    \"slotbaru555 gacor\",\n",
    "    \"nonton film gratis di youtube channel ini\"\n",
    "]\n",
    "\n",
    "print(\"\\n===== TEST RESULTS =====\")\n",
    "for t in tests:\n",
    "    print(predict(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ff4d20c-e297-49c3-afa9-afd354f84cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading real dataset ---\n",
      "Total samples: 26135\n",
      "target\n",
      "0    18872\n",
      "1     7263\n",
      "Name: count, dtype: int64\n",
      "Epoch 1/15\n",
      "\u001b[1m130/425\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:38\u001b[0m 333ms/step - accuracy: 0.7896 - loss: 0.0564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 257\u001b[0m\n\u001b[1;32m    246\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m    247\u001b[0m     Embedding(MAX_WORDS, EMBED_DIM),\n\u001b[1;32m    248\u001b[0m     Bidirectional(LSTM(\u001b[38;5;241m128\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    253\u001b[0m ])\n\u001b[1;32m    255\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mfocal_loss(), optimizer\u001b[38;5;241m=\u001b[39mAdam(LR), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 257\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    258\u001b[0m     X_res, Y_res,\n\u001b[1;32m    259\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m    260\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m    261\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m    262\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    263\u001b[0m         EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    264\u001b[0m         ReduceLROnPlateau(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    265\u001b[0m     ],\n\u001b[1;32m    266\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39msave(MODEL_PATH)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# EVALUATION\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# =========================\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    398\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 399\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    400\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    239\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    240\u001b[0m     ):\n\u001b[0;32m--> 241\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    243\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1689\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1690\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1691\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1692\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1693\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1694\u001b[0m   )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# file: robust_judol_training_and_prediction.py\n",
    "# Robust model + augmentation + negative contrastive samples (fix false positive JP/WD)\n",
    "\n",
    "import os, re, random, unicodedata, string, pickle\n",
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "FILE_PATH = \"final_production_judol_detection.csv\"\n",
    "MAX_WORDS = 30000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "TEST_SIZE_FINAL = 0.20\n",
    "NEW_ALPHA = 0.55\n",
    "OPTIMAL_THRESHOLD = 0.65\n",
    "TEXT_COLUMN = 'combined_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "\n",
    "TOKENIZER_PATH = \"tokenizer_augmented_robust.pickle\"\n",
    "MODEL_PATH = \"judol_detection_augmented_smote_robust.keras\"\n",
    "\n",
    "# =========================\n",
    "# CLEANING HELPERS\n",
    "# =========================\n",
    "ZERO_WIDTH_CHARS = ['\\u200b','\\u200c','\\u200d','\\u2060','\\uFEFF']\n",
    "def remove_zero_width(t): \n",
    "    for z in ZERO_WIDTH_CHARS: t = t.replace(z,'')\n",
    "    return t\n",
    "\n",
    "def normalize_unicode(t):\n",
    "    t = unicodedata.normalize('NFKD', t)\n",
    "    return t.encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "SEP_CHARS = r\"[\\s\\.\\-_\\|‚Ä¢¬∑\\u2063]\"\n",
    "MIN_MERGE_LEN = 3\n",
    "\n",
    "def merge_spaced_characters_simple(text, min_len=3):\n",
    "    tokens = text.split()\n",
    "    merged, buffer = [], []\n",
    "    for t in tokens:\n",
    "        if len(t)==1 and re.match(r'[A-Za-z0-9]', t):\n",
    "            buffer.append(t)\n",
    "        else:\n",
    "            if len(buffer)>=min_len: merged.append(\"\".join(buffer))\n",
    "            else: merged.extend(buffer)\n",
    "            buffer=[]\n",
    "            merged.append(t)\n",
    "    if buffer:\n",
    "        if len(buffer)>=min_len: merged.append(\"\".join(buffer))\n",
    "        else: merged.extend(buffer)\n",
    "    return \" \".join(merged)\n",
    "\n",
    "def merge_obfuscated_sequences(text, min_len=3):\n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "    out, buffer = [], []\n",
    "    for tok in tokens:\n",
    "        if tok.isspace():\n",
    "            if buffer:\n",
    "                if len(buffer)>=min_len:\n",
    "                    merged = re.sub(SEP_CHARS,'',\"\".join(buffer))\n",
    "                    out.append(merged)\n",
    "                else: out.append(\"\".join(buffer))\n",
    "                buffer=[]\n",
    "            out.append(tok)\n",
    "            continue\n",
    "        if re.fullmatch(r'(?:[A-Za-z0-9]|'+SEP_CHARS+r')+$', tok):\n",
    "            buffer.append(tok)\n",
    "        else:\n",
    "            if buffer:\n",
    "                if len(buffer)>=min_len:\n",
    "                    out.append(re.sub(SEP_CHARS,'',\"\".join(buffer)))\n",
    "                else: out.append(\"\".join(buffer))\n",
    "                buffer=[]\n",
    "            out.append(tok)\n",
    "    if buffer:\n",
    "        if len(buffer)>=min_len: out.append(re.sub(SEP_CHARS,'',\"\".join(buffer)))\n",
    "        else: out.append(\"\".join(buffer))\n",
    "    return \"\".join(out)\n",
    "\n",
    "def clean_text_unified(text):\n",
    "    if not isinstance(text,str): text=str(text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\",\" \",text)\n",
    "    text = remove_zero_width(text)\n",
    "    text = normalize_unicode(text)\n",
    "    text = merge_spaced_characters_simple(text)\n",
    "    text = merge_obfuscated_sequences(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\",\" \",text)\n",
    "    text = re.sub(r\"\\s+\",\" \",text).strip()\n",
    "    return text\n",
    "\n",
    "# =========================\n",
    "# DATASET LOAD\n",
    "# =========================\n",
    "print(\"--- Loading real dataset ---\")\n",
    "df_original = pd.read_csv(FILE_PATH)\n",
    "df_original = df_original.dropna(subset=[TEXT_COLUMN, TARGET_COLUMN])\n",
    "df_original[TARGET_COLUMN] = df_original[TARGET_COLUMN].astype(int)\n",
    "df_original[TEXT_COLUMN] = df_original[TEXT_COLUMN].astype(str).apply(clean_text_unified)\n",
    "\n",
    "# =========================\n",
    "# SYNTHETIC POSITIVE (obfuscation)\n",
    "# =========================\n",
    "prefixes = ['slot','jp','wd','depo','gacor','raja','bosku','mantap','pulau','vip','maxwin','king','boss']\n",
    "numbers = [str(i) for i in range(1, 500)]\n",
    "phrases = [\n",
    "    'pasti jp di sini','wd cepat cair','deposito langsung','gacor banget',\n",
    "    'langsung wd tanpa ribet','buruan daftar','bonus untuk depo'\n",
    "]\n",
    "\n",
    "def generate_obfuscations(s):\n",
    "    out=set()\n",
    "    out.add(s); out.add(s.lower())\n",
    "    def spaced(x): return \" \".join(list(x))\n",
    "    def dotted(x): return \".\".join(list(x))\n",
    "    def us(x): return \"_\".join(list(x))\n",
    "    def leet(x): return x.translate(str.maketrans({'a':'4','o':'0','i':'1','e':'3','s':'5','t':'7'}))\n",
    "    out.update({\n",
    "        spaced(s), dotted(s), us(s), leet(s),\n",
    "    })\n",
    "    return list(out)[:6]\n",
    "\n",
    "synthetic_rows=[]\n",
    "for _ in range(1200):\n",
    "    brand = random.choice(prefixes)+random.choice(numbers)\n",
    "    phrase = random.choice(phrases)\n",
    "    variants = generate_obfuscations(brand)\n",
    "    for v in variants:\n",
    "        form = random.choice([\n",
    "            f\"{phrase} {v}\",\n",
    "            f\"buruan {v} jp\",\n",
    "            f\"wd cepat di {v}\",\n",
    "            f\"main di {v} aja\",\n",
    "            f\"{v} gacor jp wd\"\n",
    "        ])\n",
    "        cleaned = clean_text_unified(form)\n",
    "        synthetic_rows.append({\n",
    "            \"comment_text\": form,\n",
    "            \"cleaned_comment_text\": cleaned,\n",
    "            \"target\": 1,\n",
    "            TEXT_COLUMN: cleaned\n",
    "        })\n",
    "\n",
    "df_synth = pd.DataFrame(synthetic_rows)\n",
    "\n",
    "# =========================\n",
    "# NEGATIVE CONTRASTIVE (fix false-positive JP/WD/DEPO)\n",
    "# =========================\n",
    "negative_phrases = [\n",
    "    \"akhirnya jp di turnamen badminton nasional\",\n",
    "    \"tim itu berhasil jp di pertandingan final\",\n",
    "    \"wd laptop saya ke service center\",\n",
    "    \"wd foto ke harddisk sebelum format\",\n",
    "    \"lagi depo uang ke rekening tabungan\",\n",
    "    \"depo ke rekening perusahaan untuk bayar vendor\",\n",
    "    \"dapet bonus dari kampus karena nilai bagus\",\n",
    "    \"bonus akhir tahun dari kantor turun\",\n",
    "    \"mantap banget videonya bang\",\n",
    "    \"mantap penjelasannya dosen\",\n",
    "    \"jp disini juara pertama bukan slot\",\n",
    "    \"wd tugas kuliah dulu baru tidur\",\n",
    "    \"depo file ke cloud biar aman\",\n",
    "    \"bonus track lagu baru band itu keren\"\n",
    "]\n",
    "\n",
    "neg_rows=[]\n",
    "for txt in negative_phrases:\n",
    "    c = clean_text_unified(txt)\n",
    "    neg_rows.append({\n",
    "        \"comment_text\": txt,\n",
    "        \"cleaned_comment_text\": c,\n",
    "        \"target\": 0,\n",
    "        TEXT_COLUMN: c\n",
    "    })\n",
    "\n",
    "df_negative = pd.DataFrame(neg_rows)\n",
    "\n",
    "# =========================\n",
    "# COMBINE DATA\n",
    "# =========================\n",
    "df_combined = pd.concat([\n",
    "    df_original[['comment_text','cleaned_comment_text','target',TEXT_COLUMN]],\n",
    "    df_synth,\n",
    "    df_negative\n",
    "], ignore_index=True)\n",
    "\n",
    "df_combined = df_combined[df_combined[TEXT_COLUMN]!=\"\"]\n",
    "print(\"Total samples:\", len(df_combined))\n",
    "print(df_combined['target'].value_counts())\n",
    "\n",
    "# =========================\n",
    "# TRAIN/TEST SPLIT\n",
    "# =========================\n",
    "X = df_combined[TEXT_COLUMN]\n",
    "Y = df_combined['target'].astype(int)\n",
    "\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=TEST_SIZE_FINAL, random_state=SEED, stratify=Y\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# TOKENIZER\n",
    "# =========================\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "with open(TOKENIZER_PATH,'wb') as f: pickle.dump(tokenizer,f)\n",
    "\n",
    "X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=MAX_LEN)\n",
    "X_test_pad = pad_sequences(tokenizer.texts_to_sequences(X_test_text), maxlen=MAX_LEN)\n",
    "\n",
    "# =========================\n",
    "# SMOTE\n",
    "# =========================\n",
    "sm = SMOTE()\n",
    "X_res, Y_res = sm.fit_resample(X_train_pad, Y_train)\n",
    "\n",
    "# =========================\n",
    "# MODEL\n",
    "# =========================\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    def loss(y_true,y_pred):\n",
    "        y_true=tf.cast(y_true,tf.float32)\n",
    "        y_pred=tf.clip_by_value(tf.cast(y_pred,tf.float32),1e-7,1-1e-7)\n",
    "        return tf.reduce_mean(\n",
    "            -alpha*y_true*(1-y_pred)**gamma*tf.math.log(y_pred)\n",
    "            -(1-alpha)*(1-y_true)*(y_pred**gamma)*tf.math.log(1-y_pred)\n",
    "        )\n",
    "    return loss\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(MAX_WORDS, EMBED_DIM),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=focal_loss(), optimizer=Adam(LR), metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_res, Y_res,\n",
    "    validation_split=0.1,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=4, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(patience=2, factor=0.5)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(MODEL_PATH)\n",
    "\n",
    "# =========================\n",
    "# EVALUATION\n",
    "# =========================\n",
    "pred_prob = model.predict(X_test_pad)\n",
    "pred = (pred_prob>OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "print(classification_report(Y_test, pred))\n",
    "\n",
    "# =========================\n",
    "# PREDICTION UTIL\n",
    "# =========================\n",
    "def load_artifacts(model_path=MODEL_PATH, tokenizer_path=TOKENIZER_PATH):\n",
    "    with open(tokenizer_path,'rb') as f: tk = pickle.load(f)\n",
    "    mdl = load_model(model_path, custom_objects={'loss':focal_loss()})\n",
    "    return mdl, tk\n",
    "\n",
    "def predict_single(text, mdl, tk, threshold=OPTIMAL_THRESHOLD):\n",
    "    clean = clean_text_unified(text)\n",
    "    seq = pad_sequences(tk.texts_to_sequences([clean]),maxlen=MAX_LEN)\n",
    "    prob = mdl.predict(seq,verbose=0)[0][0]\n",
    "    return {\"input\": text,\"clean\": clean,\"prob\": float(prob),\"label\": int(prob>threshold)}\n",
    "\n",
    "print(\"\\nQuick test:\")\n",
    "mdl, tk = load_artifacts()\n",
    "tests = [\n",
    "    \"wd tugas kuliah dulu ya bang\",\n",
    "    \"jp final badminton nasional\",\n",
    "    \"p.u.l.a.u.w.i.n.8.8 gacor wd\",\n",
    "    \"nonton film gratis di youtube\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(predict_single(t, mdl, tk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4db4a4cd-acf8-4fa2-a33a-d4cbd4a64143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts loaded. Ready to test.\n",
      "\n",
      "=== EXAMPLE PREDICTIONS ===\n",
      "{'input': 'Dajjal kalo liat cara fitnah ARMY ini.. pasti sungkem sujud sujud ', 'cleaned': 'dajjal kalo liat cara fitnah army ini pasti sungkem sujud sujud', 'prob': 0.9538877010345459, 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# file: test_robust_judol_model.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "MAX_LEN = 100\n",
    "NEW_ALPHA = 0.55\n",
    "OPTIMAL_THRESHOLD = 0.65\n",
    "\n",
    "TOKENIZER_PATH = \"tokenizer_augmented_robust.pickle\"\n",
    "MODEL_PATH = \"judol_detection_augmented_smote_robust.keras\"\n",
    "\n",
    "ZERO_WIDTH_CHARS = [\n",
    "    '\\u200b', '\\u200c', '\\u200d', '\\u2060', '\\uFEFF'\n",
    "]\n",
    "\n",
    "SEP_CHARS = r\"[\\s\\.\\-_\\|‚Ä¢¬∑\\u2063]\"\n",
    "MIN_MERGE_LEN = 3\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CLEANING PIPELINE (SAMA PERSIS DENGAN TRAINING)\n",
    "# =========================\n",
    "\n",
    "def remove_zero_width(text: str) -> str:\n",
    "    for z in ZERO_WIDTH_CHARS:\n",
    "        text = text.replace(z, '')\n",
    "    return text\n",
    "\n",
    "def normalize_unicode(text: str) -> str:\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def merge_spaced_characters_simple(text: str, min_len=MIN_MERGE_LEN) -> str:\n",
    "    tokens = text.split()\n",
    "    merged = []\n",
    "    buffer = []\n",
    "    for t in tokens:\n",
    "        if len(t) == 1 and re.match(r'[A-Za-z0-9]', t):\n",
    "            buffer.append(t)\n",
    "        else:\n",
    "            if len(buffer) >= min_len:\n",
    "                merged.append(\"\".join(buffer))\n",
    "            elif buffer:\n",
    "                merged.extend(buffer)\n",
    "            buffer = []\n",
    "            merged.append(t)\n",
    "    if buffer:\n",
    "        if len(buffer) >= min_len:\n",
    "            merged.append(\"\".join(buffer))\n",
    "        else:\n",
    "            merged.extend(buffer)\n",
    "    return \" \".join(merged)\n",
    "\n",
    "def merge_obfuscated_sequences(text: str, min_len=MIN_MERGE_LEN) -> str:\n",
    "    tokens = re.split(r'(\\s+)', text)\n",
    "    out_tokens = []\n",
    "    buffer = []\n",
    "    for tok in tokens:\n",
    "        if tok.isspace():\n",
    "            if buffer:\n",
    "                merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "                out_tokens.append(merged)\n",
    "                buffer = []\n",
    "            out_tokens.append(tok)\n",
    "            continue\n",
    "        if re.fullmatch(r'(?:[A-Za-z0-9]|' + SEP_CHARS + r')+$', tok):\n",
    "            buffer.append(tok)\n",
    "        else:\n",
    "            if buffer:\n",
    "                merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "                out_tokens.append(merged)\n",
    "                buffer = []\n",
    "            out_tokens.append(tok)\n",
    "    if buffer:\n",
    "        merged = re.sub(SEP_CHARS, '', \"\".join(buffer))\n",
    "        out_tokens.append(merged)\n",
    "    return \"\".join(out_tokens)\n",
    "\n",
    "def clean_text_unified(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \" \", text)\n",
    "    text = remove_zero_width(text)\n",
    "    text = normalize_unicode(text)\n",
    "    text = merge_spaced_characters_simple(text)\n",
    "    text = merge_obfuscated_sequences(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD ARTIFACTS\n",
    "# =========================\n",
    "\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    import tensorflow as tf\n",
    "    def loss(y_true, y_pred):\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        return -alpha * y_true * ((1 - y_pred) ** gamma) * tf.math.log(y_pred) - \\\n",
    "               (1 - alpha) * (1 - y_true) * (y_pred ** gamma) * tf.math.log(1 - y_pred)\n",
    "    return loss\n",
    "\n",
    "def load_artifacts():\n",
    "    if not os.path.exists(TOKENIZER_PATH):\n",
    "        raise FileNotFoundError(f\"Tokenizer not found: {TOKENIZER_PATH}\")\n",
    "\n",
    "    with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "        tokenizer = pickle.load(f)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "\n",
    "    model = load_model(MODEL_PATH, custom_objects={\"loss\": focal_loss()})\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PREDICT SINGLE / BATCH\n",
    "# =========================\n",
    "\n",
    "def predict_single(text, model, tokenizer, threshold=OPTIMAL_THRESHOLD):\n",
    "    cleaned = clean_text_unified(text)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    prob = model.predict(pad, verbose=0)[0][0]\n",
    "    label = int(prob > threshold)\n",
    "    return {\n",
    "        \"input\": text,\n",
    "        \"cleaned\": cleaned,\n",
    "        \"prob\": float(prob),\n",
    "        \"label\": label\n",
    "    }\n",
    "\n",
    "def predict_batch(list_text, model, tokenizer, threshold=OPTIMAL_THRESHOLD):\n",
    "    cleaned = [clean_text_unified(t) for t in list_text]\n",
    "    seq = tokenizer.texts_to_sequences(cleaned)\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    probs = model.predict(pad, verbose=0).reshape(-1)\n",
    "    labels = (probs > threshold).astype(int)\n",
    "    result = []\n",
    "    for raw, clean, prob, lbl in zip(list_text, cleaned, probs, labels):\n",
    "        result.append({\n",
    "            \"input\": raw,\n",
    "            \"cleaned\": clean,\n",
    "            \"prob\": float(prob),\n",
    "            \"label\": int(lbl)\n",
    "        })\n",
    "    return result\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CLI DEMO (run: python test_robust_judol_model.py)\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_artifacts()\n",
    "    print(\"Artifacts loaded. Ready to test.\\n\")\n",
    "\n",
    "    sample = [\n",
    "        \"Dajjal kalo liat cara fitnah ARMY ini.. pasti sungkem sujud sujud \"\n",
    "    ]\n",
    "\n",
    "    print(\"=== EXAMPLE PREDICTIONS ===\")\n",
    "    out = predict_batch(sample, model, tokenizer)\n",
    "    for r in out:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb220a-ba11-49d8-b023-85ef380f392e",
   "metadata": {},
   "source": [
    "# ya pakai clena text sama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "385da5d2-ae41-427d-a4d2-ef0e2cb99ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat Model dari judol_detection_augmented_smote.h5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat Tokenizer dari tokenizer_augmented.pickle ---\n",
      "\n",
      "‚úÖ Model dan Tokenizer berhasil dimuat.\n",
      "\n",
      "=============================================\n",
      "üöÄ UJI COBA MODEL DETEKSI JUDOL REAL-TIME (Threshold=0.65)\n",
      "=============================================\n",
      "\n",
      "üí¨ Input: 'WD 500rb di GACOR777 mantap langsung cair!'\n",
      "üßπ Clean: 'wd 500rb di gacor777 mantap langsung cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7679)\n",
      "\n",
      "üí¨ Input: 'buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!'\n",
      "üßπ Clean: 'buruan daftar di mantapjp888 depo 100k langsung dapat bonus'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5325)\n",
      "\n",
      "üí¨ Input: 'main slot pasti untung, depo terus bosku'\n",
      "üßπ Clean: 'main slot pasti untung depo terus bosku'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7273)\n",
      "\n",
      "üí¨ Input: 'BOSKU WD 5 JUTA HARI INI DI SITUS RAJA88.'\n",
      "üßπ Clean: 'bosku wd 5 juta hari ini di situs raja88'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8439)\n",
      "\n",
      "üí¨ Input: 'S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.'\n",
      "üßπ Clean: 's l 0 t p r 1 m a 8 8 8 jp 100k wd cepat'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8439)\n",
      "\n",
      "üí¨ Input: 'Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.'\n",
      "üßπ Clean: 'cepat gabung w1n k3lub b4rus4n w d s4ld0 5j'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1602)\n",
      "\n",
      "üí¨ Input: 'Lagi gacor main di maxwin888 hari ini.'\n",
      "üßπ Clean: 'lagi gacor main di maxwin888 hari ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2391)\n",
      "\n",
      "üí¨ Input: 'Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞'\n",
      "üßπ Clean: 'top banget terimakasih 55 good job'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1798)\n",
      "\n",
      "üí¨ Input: 'WD 1O jt di BOSSku777. *lgsg cair*!'\n",
      "üßπ Clean: 'wd 1o jt di bossku777 lgsg cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7536)\n",
      "\n",
      "üí¨ Input: 'Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!'\n",
      "üßπ Clean: 'depo 100k wd 20juta di rajaslot777 pasti jp'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7019)\n",
      "\n",
      "üí¨ Input: 'Aku di MANTAP888. Gacor terus!'\n",
      "üßπ Clean: 'aku di mantap888 gacor terus'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3464)\n",
      "\n",
      "üí¨ Input: 'buruan daftar di MANTAPJEPE888 promonya gede banget'\n",
      "üßπ Clean: 'buruan daftar di mantapjepe888 promonya gede banget'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3983)\n",
      "\n",
      "üí¨ Input: 'Paling the best! HOTJEPE55 Memang GOOD JOB banget!'\n",
      "üßπ Clean: 'paling the best hotjepe55 memang good job banget'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1247)\n",
      "\n",
      "üí¨ Input: 'main di slotvip4d aja, depo 50 wd 500'\n",
      "üßπ Clean: 'main di slotvip4d aja depo 50 wd 500'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7408)\n",
      "\n",
      "üí¨ Input: 'gacorwd77 adalah situs paling oke.'\n",
      "üßπ Clean: 'gacorwd77 adalah situs paling oke'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4809)\n",
      "\n",
      "üí¨ Input: 'wd 1000 jp. link ada di bio ya.'\n",
      "üßπ Clean: 'wd 1000 jp link ada di bio ya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6113)\n",
      "\n",
      "üí¨ Input: 'jp 2OOm jt sudah di WD. Paling gacor di sini.'\n",
      "üßπ Clean: 'jp 2oom jt sudah di wd paling gacor di sini'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6596)\n",
      "\n",
      "üí¨ Input: 'S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok'\n",
      "üßπ Clean: 's l o t g a c o r 1 2 3 c o m puji tuhan rejeki nomplok'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6148)\n",
      "\n",
      "üí¨ Input: 'asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.'\n",
      "üßπ Clean: 'asli keren 688 selalu bikin hepyy'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1750)\n",
      "\n",
      "üí¨ Input: 'tokowin303 4w1 likeReply'\n",
      "üßπ Clean: 'tokowin303 4w1 likereply'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1387)\n",
      "\n",
      "üí¨ Input: 'Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta'\n",
      "üßπ Clean: 'saya mau ngasih saran yg baik ya bg kemaren saya main di 404 di kasih 22juta'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.0742)\n",
      "\n",
      "üí¨ Input: 'ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan'\n",
      "üßπ Clean: '91 emang the real mvp selalu bikin kita bahagia dengan kemenangan emang the real mvp selalu bikin kita bahagia dengan kemenangan'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3109)\n",
      "\n",
      "üí¨ Input: 'slotvip777 depo 50 wd 500'\n",
      "üßπ Clean: 'slotvip777 depo 50 wd 500'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8227)\n",
      "\n",
      "üí¨ Input: 'JP boss 44D mantap'\n",
      "üßπ Clean: 'jp boss 44d mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1638)\n",
      "\n",
      "üí¨ Input: 'üí∞ WD 5üü¢0üü¢0k di P U L A U W I N 88! Langsung ü§ë JP!'\n",
      "üßπ Clean: 'wd 500k di p u l a u w i n 88 langsung jp'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8005)\n",
      "\n",
      "üí¨ Input: 'S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat'\n",
      "üßπ Clean: 's l 0 t g a c 0 r 1 2 3 jp 500k wd cepat'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8204)\n",
      "\n",
      "üí¨ Input: 'ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j'\n",
      "üßπ Clean: '88 wd 200j'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6406)\n",
      "\n",
      "üí¨ Input: 'W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!'\n",
      "üßπ Clean: 'w d 1 0 j t d i u777'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.0870)\n",
      "\n",
      "üí¨ Input: 'jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!'\n",
      "üßπ Clean: 'jp besar wd kecil main di s l 0 t v 1 p 1 1 1 sekarang'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.9374)\n",
      "\n",
      "üí¨ Input: 'p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!'\n",
      "üßπ Clean: 'p l w n88 jp 10j cepat wd'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7712)\n",
      "\n",
      "üí¨ Input: 'garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar'\n",
      "üßπ Clean: 'garudahoki mantapjepe888 jp 100k wd lancar'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.9176)\n",
      "\n",
      "üí¨ Input: 'wd sepuluh juta di slotvip111 üéØ langsung cair'\n",
      "üßπ Clean: 'wd sepuluh juta di slotvip111 langsung cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8117)\n",
      "\n",
      "üí¨ Input: 'wd cepat + JP 200k main di RAJASLOT777 sekarang'\n",
      "üßπ Clean: 'wd cepat jp 200k main di rajaslot777 sekarang'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6509)\n",
      "\n",
      "üí¨ Input: 'Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.'\n",
      "üßπ Clean: 'daft4r di s g 1 88 bentar lagi ada drop mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1451)\n",
      "\n",
      "üí¨ Input: 'mAin di bùöòùöúÔªúùöû7ùüΩùüΩ enak bet, wd aman.'\n",
      "üßπ Clean: 'main di bosu777 enak bet wd aman'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6383)\n",
      "\n",
      "üí¨ Input: 'bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.'\n",
      "üßπ Clean: 'bo s ku tujuh tujuh tujuh lagi gila2an promo wdee'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2383)\n",
      "\n",
      "üí¨ Input: 'm‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.'\n",
      "üßπ Clean: 'main di bosku777 gasken'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1515)\n",
      "\n",
      "üí¨ Input: 'Coba deh di \"QlRTT1QxMDA=\" katanya jam 3 pagi gacor.'\n",
      "üßπ Clean: 'coba deh di qlrtt1qxmda katanya jam 3 pagi gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4041)\n",
      "\n",
      "üí¨ Input: 'Main di cptlv888 lumayan tarikannya.'\n",
      "üßπ Clean: 'main di cptlv888 lumayan tarikannya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1810)\n",
      "\n",
      "üí¨ Input: 'Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.'\n",
      "üßπ Clean: ''\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3225)\n",
      "\n",
      "üí¨ Input: 'b0sku77? gas bet wd ngebut bgt.'\n",
      "üßπ Clean: 'b0sku77 gas bet wd ngebut bgt'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6850)\n",
      "\n",
      "üí¨ Input: 'b o s k u\n",
      "7 7 7\n",
      "...disusun vertikal.'\n",
      "üßπ Clean: 'b o s k u 7 7 7 disusun vertikal'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4776)\n",
      "\n",
      "üí¨ Input: 'WD 600 tadi, no cap. That site hits diff fr.'\n",
      "üßπ Clean: 'wd 600 tadi no cap that site hits diff fr'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5422)\n",
      "\n",
      "üí¨ Input: 'M@!N di B0$KU^777 cuy wd kuzz teroosss.'\n",
      "üßπ Clean: 'm n di b0 ku 777 cuy wd kuzz teroosss'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6465)\n",
      "\n",
      "üí¨ Input: 'bo sku 7 7 7 ‚Äî karena kamera burem jd gini.'\n",
      "üßπ Clean: 'bo sku 7 7 7 karena kamera burem jd gini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4578)\n",
      "\n",
      "üí¨ Input: 'Beli skin game baru ah, keren nih!'\n",
      "üßπ Clean: 'beli skin game baru ah keren nih'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3126)\n",
      "\n",
      "üí¨ Input: 'Ini channel YouTube favoritku, sering update video baru.'\n",
      "üßπ Clean: 'ini channel youtube favoritku sering update video baru'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2540)\n",
      "\n",
      "üí¨ Input: 'Makan malam apa hari ini?'\n",
      "üßπ Clean: 'makan malam apa hari ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2534)\n",
      "\n",
      "üí¨ Input: 'Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan'\n",
      "üßπ Clean: 'saya mau nonton film gratis di y0utube tapi juga wd 5 ribu dari tabungan'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6907)\n",
      "\n",
      "üí¨ Input: 'WD 50 ribu saja, buat beli kopi.'\n",
      "üßπ Clean: 'wd 50 ribu saja buat beli kopi'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6935)\n",
      "\n",
      "üí¨ Input: 'wah mantap nih, motor baru udah di WD sama pemiliknya'\n",
      "üßπ Clean: 'wah mantap nih motor baru udah di wd sama pemiliknya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4296)\n",
      "\n",
      "üí¨ Input: 'itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.'\n",
      "üßπ Clean: 'itu tempat bosku yang warna hijau itu loh yg sering iklan jam 2 pagi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4318)\n",
      "\n",
      "üí¨ Input: 'Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.'\n",
      "üßπ Clean: 'tadi malem aku pecah telur di tempat sebelah tarikan bersih bgt'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2606)\n",
      "\n",
      "üí¨ Input: 'Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.'\n",
      "üßπ Clean: 'udah masuk aja ke yang kemarin aku bilang yang tiap hari bagi hadiah itu'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1726)\n",
      "\n",
      "üí¨ Input: 'Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°'\n",
      "üßπ Clean: 'wkwk mantap jp lagi di platform terpercaya itu katanya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4236)\n",
      "\n",
      "üí¨ Input: 'bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.'\n",
      "üßπ Clean: 'bos ku sudah datang tadi pagi bawa hadiah 25k gila'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3169)\n",
      "\n",
      "üí¨ Input: 'Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.'\n",
      "üßπ Clean: 'ingat keberanian itu modal kalau mau hasil besar tempat itu udah aku rekomendasiin kemarin'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4233)\n",
      "\n",
      "üí¨ Input: 'Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!'\n",
      "üßπ Clean: 'semoga sukses selalu dan sehat terus ya kontennya sangat bermanfaat'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4994)\n",
      "\n",
      "üí¨ Input: 'Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.'\n",
      "üßπ Clean: 'saya transfer 500 ribu ke rekening teman tadi pagi semoga lancar'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4433)\n",
      "\n",
      "üí¨ Input: 'Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.'\n",
      "üßπ Clean: 'main mobile legends seru banget tapi harus top up dulu buat beli diamond'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4518)\n",
      "\n",
      "üí¨ Input: 'Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.'\n",
      "üßπ Clean: 'saya pesan nasi padang di warung bossku tadi rasanya mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2149)\n",
      "\n",
      "üí¨ Input: 'Saya sudah JP (Jelas Puas) sama pelayanan toko ini.'\n",
      "üßπ Clean: 'saya sudah jp jelas puas sama pelayanan toko ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5520)\n",
      "\n",
      "üí¨ Input: 'Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.'\n",
      "üßπ Clean: 'komentar ini hanya berisi pujian tidak ada unsur promosi atau judi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2742)\n",
      "\n",
      "üí¨ Input: 'link untuk download materi kuliah ada di deskripsi video ya teman-teman.'\n",
      "üßπ Clean: 'link untuk download materi kuliah ada di deskripsi video ya teman teman'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2333)\n",
      "\n",
      "üí¨ Input: 'Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.'\n",
      "üßπ Clean: 'terima kasih atas video tutorial cara wd withdrawal uang dari platform saham'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6925)\n",
      "\n",
      "üí¨ Input: 'Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè'\n",
      "üßπ Clean: 'dijamin happy udah kaya rumah kedua 923 disini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2130)\n",
      "\n",
      "üí¨ Input: 'cuma di tempat lu bang yang paling amanahüôå'\n",
      "üßπ Clean: 'cuma di tempat lu bang yang paling amanah'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1210)\n",
      "\n",
      "üí¨ Input: 'ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya'\n",
      "üßπ Clean: '189 emang the real mvp selalu bikin kita bahagia dengan kemenangannya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2564)\n",
      "\n",
      "üí¨ Input: 'Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.'\n",
      "üßπ Clean: 'tadi pagi aku wd withdraw uang tunai dari atm 500 ribu'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6052)\n",
      "\n",
      "üí¨ Input: 'WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.'\n",
      "üßπ Clean: 'wd dana beasiswa sudah cair total 5 juta alhamdulillah'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8831)\n",
      "\n",
      "üí¨ Input: 'Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.'\n",
      "üßπ Clean: 'jangan lupa wd hasil penjualan dari platform e commerce itu ya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5996)\n",
      "\n",
      "üí¨ Input: 'Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!'\n",
      "üßπ Clean: 'pelayanan toko ini jp jelas puas banget barangnya gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5000)\n",
      "\n",
      "üí¨ Input: 'Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.'\n",
      "üßπ Clean: 'aku jp jaminan produk 100 kalau kamu beli di bossku official store'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2785)\n",
      "\n",
      "üí¨ Input: 'Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!'\n",
      "üßπ Clean: 'wah si bossku tadi pagi datang bawa hadiah mantap jp'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4891)\n",
      "\n",
      "üí¨ Input: 'Motor baruku Gacor banget, tarikannya mantap!'\n",
      "üßπ Clean: 'motor baruku gacor banget tarikannya mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1483)\n",
      "\n",
      "üí¨ Input: 'Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.'\n",
      "üßπ Clean: 'si bossku panggilan teman tadi depo deposit makanan ke meja'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5816)\n",
      "\n",
      "üí¨ Input: 'Restoran ini Gacor tiap hari, ramenya nggak ketulungan.'\n",
      "üßπ Clean: 'restoran ini gacor tiap hari ramenya nggak ketulungan'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3495)\n",
      "\n",
      "üí¨ Input: 'Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).'\n",
      "üßπ Clean: 'aku dapat promo depo 50k dapat 50k di toko x bukan situs judi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6081)\n",
      "\n",
      "üí¨ Input: 'Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).'\n",
      "üßπ Clean: 'gacor banget main game mobile legends dapat jp juara pertama'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6869)\n",
      "\n",
      "üí¨ Input: 'Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).'\n",
      "üßπ Clean: 'kata bossku ini adalah situs terbaik untuk beli akun game bukan judi'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6504)\n",
      "\n",
      "üí¨ Input: 'WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.'\n",
      "üßπ Clean: 'wd 5 ribu aja buat beli kopi soalnya dompet udah gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5541)\n",
      "\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI PENGUJIAN\n",
    "# ===============================================\n",
    "MODEL_PATH = \"judol_detection_augmented_smote.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer_augmented.pickle\"\n",
    "MAX_LEN = 100             # Harus sama dengan MAX_LEN saat training\n",
    "OPTIMAL_THRESHOLD = 0.65  # Threshold optimal yang Anda temukan\n",
    "\n",
    "# ===============================================\n",
    "# 1. DEFINISI FUNGSI CUSTOM (Focal Loss)\n",
    "# ===============================================\n",
    "# Focal Loss HARUS didefinisikan ulang agar Keras bisa memuat model\n",
    "def focal_loss(gamma=2., alpha=0.55): # Alpha 0.8 harus sama dengan saat training\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# ===============================================\n",
    "# 2. FUNGSI PREPROCESSING UNTUK PREDIKSI (Sesuai Training)\n",
    "# ===============================================\n",
    "def clean_text_for_prediction(text):\n",
    "    \"\"\"Fungsi cleaning yang digunakan saat prediksi (dengan normalisasi Unicode yang lebih robust)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    # Normalisasi Unicode\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') \n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# ===============================================\n",
    "# 3. FUNGSI UTAMA PREDIKSI\n",
    "# ===============================================\n",
    "def predict_judol(text, model, tokenizer, threshold=OPTIMAL_THRESHOLD):\n",
    "    \"\"\"Memproses teks dan mengembalikan probabilitas dan label prediksi Judol.\"\"\"\n",
    "    \n",
    "    text_clean = clean_text_for_prediction(text)\n",
    "    \n",
    "    # 3.1 Tokenisasi dan Padding\n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    # 3.2 Prediksi Model\n",
    "    # model.predict mengembalikan array 2D [[probabilitas]]\n",
    "    # Matikan verbose agar output bersih\n",
    "    pred_prob = model.predict(pad, verbose=0)[0][0]\n",
    "    is_judol = pred_prob > threshold\n",
    "    \n",
    "    label = \"üî• **JUDOL (SMOTE ML)**\" if is_judol else \"‚úÖ Bukan Judol (SMOTE ML)\"\n",
    "    \n",
    "    return label, pred_prob, text_clean\n",
    "\n",
    "# ===============================================\n",
    "# 4. MEMUAT MODEL DAN TOKENIZER\n",
    "# ===============================================\n",
    "try:\n",
    "    # Memuat Model\n",
    "    print(f\"--- Memuat Model dari {MODEL_PATH} ---\")\n",
    "    model_loaded = load_model(MODEL_PATH, custom_objects={'loss': focal_loss(alpha=0.8)})\n",
    "    \n",
    "    # Memuat Tokenizer\n",
    "    print(f\"--- Memuat Tokenizer dari {TOKENIZER_PATH} ---\")\n",
    "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "        tokenizer_loaded = pickle.load(handle)\n",
    "        \n",
    "    print(\"\\n‚úÖ Model dan Tokenizer berhasil dimuat.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: File model atau tokenizer tidak ditemukan. Pastikan Anda sudah menjalankan script training.\")\n",
    "    exit()\n",
    "\n",
    "# ===============================================\n",
    "# 5. UJI COBA REAL-TIME\n",
    "# ===============================================\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"üöÄ UJI COBA MODEL DETEKSI JUDOL REAL-TIME (Threshold={OPTIMAL_THRESHOLD})\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "test_comments = [\n",
    "    # --- JUDOL TESTS ---\n",
    "    \"WD 500rb di GACOR777 mantap langsung cair!\",\n",
    "    \"buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!\",\n",
    "    \"main slot pasti untung, depo terus bosku\",\n",
    "    \"BOSKU WD 5 JUTA HARI INI DI SITUS RAJA88.\",\n",
    "    \"S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.\", \n",
    "    \"Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.\",\n",
    "    \"Lagi gacor main di maxwin888 hari ini.\",\n",
    "    \"Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞\", # Uji Kasus Perbaikan\n",
    "    \"WD 1O jt di BOSSku777. *lgsg cair*!\", \n",
    "    \"Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!\",\n",
    "    \"Aku di MANTAP888. Gacor terus!\", \n",
    "    \"buruan daftar di MANTAPJEPE888 promonya gede banget\", \n",
    "    \"Paling the best! HOTJEPE55 Memang GOOD JOB banget!\",\n",
    "    \"main di slotvip4d aja, depo 50 wd 500\", \n",
    "    \"gacorwd77 adalah situs paling oke.\",\n",
    "    \"wd 1000 jp. link ada di bio ya.\", \n",
    "    \"jp 2OOm jt sudah di WD. Paling gacor di sini.\", \n",
    "    \"S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok\",\n",
    "    \"asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.\", \n",
    "    \"tokowin303 4w1 likeReply\", \n",
    "    \"Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta\", \n",
    "    \"ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan\", \n",
    "    \"slotvip777 depo 50 wd 500\", \n",
    "    \"JP boss 44D mantap\",\n",
    "    \"üí∞ WD 5üü¢0üü¢0k di P U L A U W I N 88! Langsung ü§ë JP!\", \n",
    "    \"S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat\", \n",
    "    \"ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j\", \n",
    "    \"W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!\", \n",
    "    \"jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!\", \n",
    "    \"p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!\", \n",
    "    \"garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar\", \n",
    "    \"wd sepuluh juta di slotvip111 üéØ langsung cair\", \n",
    "    \"wd cepat + JP 200k main di RAJASLOT777 sekarang\", \n",
    "    \"Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.\",\n",
    "    \"mAin di bùöòùöúÔªúùöû7ùüΩùüΩ enak bet, wd aman.\",\n",
    "    \"bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.\",\n",
    "    \"m‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.\",\n",
    "    \"Coba deh di \\\"QlRTT1QxMDA=\\\" katanya jam 3 pagi gacor.\",\n",
    "    \"Main di cptlv888 lumayan tarikannya.\",\n",
    "    \"Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.\",\n",
    "    \"b0sku77? gas bet wd ngebut bgt.\",\n",
    "    \"b o s k u\\n7 7 7\\n...disusun vertikal.\",\n",
    "    \"WD 600 tadi, no cap. That site hits diff fr.\",\n",
    "    \"M@!N di B0$KU^777 cuy wd kuzz teroosss.\",\n",
    "    \"bo sku 7 7 7 ‚Äî karena kamera burem jd gini.\",\n",
    "\n",
    "    # --- NON-JUDOL TESTS ---\n",
    "    \"Beli skin game baru ah, keren nih!\", \n",
    "    \"Ini channel YouTube favoritku, sering update video baru.\",\n",
    "    \"Makan malam apa hari ini?\",\n",
    "    \"Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan\",\n",
    "    \"WD 50 ribu saja, buat beli kopi.\", \n",
    "    \"wah mantap nih, motor baru udah di WD sama pemiliknya\", \n",
    "    \"itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.\",\n",
    "    \"Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.\", \n",
    "    \"Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.\",\n",
    "    \"Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°\",\n",
    "    \"bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.\",\n",
    "    \"Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.\",\n",
    "    \"Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!\", \n",
    "    \"Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.\", \n",
    "    \"Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.\", \n",
    "    \"Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.\", \n",
    "    \"Saya sudah JP (Jelas Puas) sama pelayanan toko ini.\", \n",
    "    \"Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.\",\n",
    "    \"link untuk download materi kuliah ada di deskripsi video ya teman-teman.\", \n",
    "    \"Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.\" ,\n",
    "    \"Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè\",\n",
    "    \"cuma di tempat lu bang yang paling amanahüôå\",\n",
    "    \"ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya\",\n",
    "    # Non-Judol menggunakan WD (Withdraw) dan Nominal\n",
    "    \"Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.\",\n",
    "    \"WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.\",\n",
    "    \"Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.\",\n",
    "    \n",
    "    # Non-Judol menggunakan JP (Jelas Puas, Jaminan Produk, etc.)\n",
    "    \"Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!\",\n",
    "    \"Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.\",\n",
    "    \"Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!\",\n",
    "\n",
    "    # Non-Judol menggunakan Gacor, Bossku, Depo (konteks non-judi)\n",
    "    \"Motor baruku Gacor banget, tarikannya mantap!\",\n",
    "    \"Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.\",\n",
    "    \"Restoran ini Gacor tiap hari, ramenya nggak ketulungan.\",\n",
    "    \n",
    "    # Non-Judol dengan Kombinasi Kata Kunci Ambigu\n",
    "    \"Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).\",\n",
    "    \"Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).\",\n",
    "    \"Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).\",\n",
    "    \"WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.\" # Gacor = Kosong\n",
    "]\n",
    "\n",
    "for comment in test_comments:\n",
    "    # Memastikan model tidak menampilkan pesan saat memprediksi\n",
    "    label, prob, clean_text = predict_judol(comment, model_loaded, tokenizer_loaded, OPTIMAL_THRESHOLD)\n",
    "    \n",
    "    print(f\"\\nüí¨ Input: '{comment}'\")\n",
    "    print(f\"üßπ Clean: '{clean_text}'\")\n",
    "    print(f\"üéØ Prediksi: {label} (Probabilitas: {prob:.4f})\")\n",
    "\n",
    "print(\"\\n=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd4344f6-d915-47ab-9361-dcb2b74c969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat Model dari judol_detection_augmented_smote.h5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memuat Tokenizer dari tokenizer_augmented.pickle ---\n",
      "\n",
      "‚úÖ Model dan Tokenizer berhasil dimuat.\n",
      "\n",
      "=============================================\n",
      "üöÄ UJI COBA MODEL DETEKSI JUDOL REAL-TIME (Threshold=0.65)\n",
      "=============================================\n",
      "\n",
      "üí¨ Input: 'WD 500rb di GACOR777 mantap langsung cair!'\n",
      "üßπ Clean: 'wd 500rb di gacor777 mantap langsung cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7679)\n",
      "\n",
      "üí¨ Input: 'buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!'\n",
      "üßπ Clean: 'buruan daftar di mantapjp888 depo 100k langsung dapat bonus'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5325)\n",
      "\n",
      "üí¨ Input: 'main slot pasti untung, depo terus bosku'\n",
      "üßπ Clean: 'main slot pasti untung depo terus bosku'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7273)\n",
      "\n",
      "üí¨ Input: 'BOSKU WD 5 JUTA HARI INI DI SITUS RAJA88.'\n",
      "üßπ Clean: 'bosku wd 5 juta hari ini di situs raja88'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8439)\n",
      "\n",
      "üí¨ Input: 'S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.'\n",
      "üßπ Clean: 's l 0 t p r 1 m a 8 8 8 jp 100k wd cepat'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8439)\n",
      "\n",
      "üí¨ Input: 'Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.'\n",
      "üßπ Clean: 'cepat gabung w1n k3lub b4rus4n w d s4ld0 5j'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1602)\n",
      "\n",
      "üí¨ Input: 'Lagi gacor main di maxwin888 hari ini.'\n",
      "üßπ Clean: 'lagi gacor main di maxwin888 hari ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2391)\n",
      "\n",
      "üí¨ Input: 'Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞'\n",
      "üßπ Clean: 'top banget terimakasih hotjepe55 good job'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1168)\n",
      "\n",
      "üí¨ Input: 'WD 1O jt di BOSSku777. *lgsg cair*!'\n",
      "üßπ Clean: 'wd 1o jt di bossku777 lgsg cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7536)\n",
      "\n",
      "üí¨ Input: 'Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!'\n",
      "üßπ Clean: 'depo 100k wd 20juta di rajaslot777 pasti jp'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7019)\n",
      "\n",
      "üí¨ Input: 'Aku di MANTAP888. Gacor terus!'\n",
      "üßπ Clean: 'aku di mantap888 gacor terus'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3464)\n",
      "\n",
      "üí¨ Input: 'buruan daftar di MANTAPJEPE888 promonya gede banget'\n",
      "üßπ Clean: 'buruan daftar di mantapjepe888 promonya gede banget'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3983)\n",
      "\n",
      "üí¨ Input: 'Paling the best! HOTJEPE55 Memang GOOD JOB banget!'\n",
      "üßπ Clean: 'paling the best hotjepe55 memang good job banget'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1247)\n",
      "\n",
      "üí¨ Input: 'main di slotvip4d aja, depo 50 wd 500'\n",
      "üßπ Clean: 'main di slotvip4d aja depo 50 wd 500'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7408)\n",
      "\n",
      "üí¨ Input: 'gacorwd77 adalah situs paling oke.'\n",
      "üßπ Clean: 'gacorwd77 adalah situs paling oke'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4809)\n",
      "\n",
      "üí¨ Input: 'wd 1000 jp. link ada di bio ya.'\n",
      "üßπ Clean: 'wd 1000 jp link ada di bio ya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6113)\n",
      "\n",
      "üí¨ Input: 'jp 2OOm jt sudah di WD. Paling gacor di sini.'\n",
      "üßπ Clean: 'jp 2oom jt sudah di wd paling gacor di sini'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6596)\n",
      "\n",
      "üí¨ Input: 'S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok'\n",
      "üßπ Clean: 's l o t g a c o r 1 2 3 c o m puji tuhan rejeki nomplok'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6148)\n",
      "\n",
      "üí¨ Input: 'asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.'\n",
      "üßπ Clean: 'asli keren galabet688 selalu bikin hepyy'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1750)\n",
      "\n",
      "üí¨ Input: 'tokowin303 4w1 likeReply'\n",
      "üßπ Clean: 'tokowin303 4w1 likereply'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1387)\n",
      "\n",
      "üí¨ Input: 'Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta'\n",
      "üßπ Clean: 'saya mau ngasih saran yg baik ya bg kemaren saya main di mantul404 di kasih 22juta'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.0742)\n",
      "\n",
      "üí¨ Input: 'ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan'\n",
      "üßπ Clean: 'laba91 emang the real mvp selalu bikin kita bahagia dengan kemenangan emang the real mvp selalu bikin kita bahagia dengan kemenangan'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3109)\n",
      "\n",
      "üí¨ Input: 'slotvip777 depo 50 wd 500'\n",
      "üßπ Clean: 'slotvip777 depo 50 wd 500'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8227)\n",
      "\n",
      "üí¨ Input: 'JP boss 44D mantap'\n",
      "üßπ Clean: 'jp boss 44d mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1638)\n",
      "\n",
      "üí¨ Input: 'üí∞ WD 5üü¢0üü¢0k di PULAUWIN 88! Langsung ü§ë JP!'\n",
      "üßπ Clean: 'wd 500k di pulauwin 88 langsung jp'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.9506)\n",
      "\n",
      "üí¨ Input: 'S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat'\n",
      "üßπ Clean: 's l 0 t g a c 0 r 1 2 3 jp 500k wd cepat'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8204)\n",
      "\n",
      "üí¨ Input: 'ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j'\n",
      "üßπ Clean: 'mantapjepe88 wd 200j'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5417)\n",
      "\n",
      "üí¨ Input: 'W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!'\n",
      "üßπ Clean: 'w d 1 0 j t d i bossku777'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.0870)\n",
      "\n",
      "üí¨ Input: 'jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!'\n",
      "üßπ Clean: 'jp besar wd kecil main di s l 0 t v 1 p 1 1 1 sekarang'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.9374)\n",
      "\n",
      "üí¨ Input: 'p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!'\n",
      "üßπ Clean: 'p l w n88 jp 10j cepat wd'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.7712)\n",
      "\n",
      "üí¨ Input: 'garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar'\n",
      "üßπ Clean: 'garudahoki mantapjepe888 jp 100k wd lancar'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.9176)\n",
      "\n",
      "üí¨ Input: 'wd sepuluh juta di slotvip111 üéØ langsung cair'\n",
      "üßπ Clean: 'wd sepuluh juta di slotvip111 langsung cair'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8117)\n",
      "\n",
      "üí¨ Input: 'wd cepat + JP 200k main di RAJASLOT777 sekarang'\n",
      "üßπ Clean: 'wd cepat jp 200k main di rajaslot777 sekarang'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6509)\n",
      "\n",
      "üí¨ Input: 'Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.'\n",
      "üßπ Clean: 'daft4r di s g 1 88 bentar lagi ada drop mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1451)\n",
      "\n",
      "üí¨ Input: 'mAin di bùöòùöúÔªúùöû7ùüΩùüΩ enak bet, wd aman.'\n",
      "üßπ Clean: 'main di bosu777 enak bet wd aman'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6383)\n",
      "\n",
      "üí¨ Input: 'bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.'\n",
      "üßπ Clean: 'bo s ku tujuh tujuh tujuh lagi gila2an promo wdee'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2383)\n",
      "\n",
      "üí¨ Input: 'm‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.'\n",
      "üßπ Clean: 'main di bosku777 gasken'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1515)\n",
      "\n",
      "üí¨ Input: 'Coba deh di \"QlRTT1QxMDA=\" katanya jam 3 pagi gacor.'\n",
      "üßπ Clean: 'coba deh di qlrtt1qxmda katanya jam 3 pagi gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4041)\n",
      "\n",
      "üí¨ Input: 'Main di cptlv888 lumayan tarikannya.'\n",
      "üßπ Clean: 'main di cptlv888 lumayan tarikannya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1810)\n",
      "\n",
      "üí¨ Input: 'Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.'\n",
      "üßπ Clean: ''\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3225)\n",
      "\n",
      "üí¨ Input: 'b0sku77? gas bet wd ngebut bgt.'\n",
      "üßπ Clean: 'b0sku77 gas bet wd ngebut bgt'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6850)\n",
      "\n",
      "üí¨ Input: 'b o s k u\n",
      "7 7 7\n",
      "...disusun vertikal.'\n",
      "üßπ Clean: 'b o s k u 7 7 7 disusun vertikal'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4776)\n",
      "\n",
      "üí¨ Input: 'WD 600 tadi, no cap. That site hits diff fr.'\n",
      "üßπ Clean: 'wd 600 tadi no cap that site hits diff fr'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5422)\n",
      "\n",
      "üí¨ Input: 'M@!N di B0$KU^777 cuy wd kuzz teroosss.'\n",
      "üßπ Clean: 'm n di b0 ku 777 cuy wd kuzz teroosss'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6465)\n",
      "\n",
      "üí¨ Input: 'bo sku 7 7 7 ‚Äî karena kamera burem jd gini.'\n",
      "üßπ Clean: 'bo sku 7 7 7 karena kamera burem jd gini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4578)\n",
      "\n",
      "üí¨ Input: 'Beli skin game baru ah, keren nih!'\n",
      "üßπ Clean: 'beli skin game baru ah keren nih'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3126)\n",
      "\n",
      "üí¨ Input: 'Ini channel YouTube favoritku, sering update video baru.'\n",
      "üßπ Clean: 'ini channel youtube favoritku sering update video baru'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2540)\n",
      "\n",
      "üí¨ Input: 'Makan malam apa hari ini?'\n",
      "üßπ Clean: 'makan malam apa hari ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2534)\n",
      "\n",
      "üí¨ Input: 'Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan'\n",
      "üßπ Clean: 'saya mau nonton film gratis di y0utube tapi juga wd 5 ribu dari tabungan'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6907)\n",
      "\n",
      "üí¨ Input: 'WD 50 ribu saja, buat beli kopi.'\n",
      "üßπ Clean: 'wd 50 ribu saja buat beli kopi'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6935)\n",
      "\n",
      "üí¨ Input: 'wah mantap nih, motor baru udah di WD sama pemiliknya'\n",
      "üßπ Clean: 'wah mantap nih motor baru udah di wd sama pemiliknya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4296)\n",
      "\n",
      "üí¨ Input: 'itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.'\n",
      "üßπ Clean: 'itu tempat bosku yang warna hijau itu loh yg sering iklan jam 2 pagi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4318)\n",
      "\n",
      "üí¨ Input: 'Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.'\n",
      "üßπ Clean: 'tadi malem aku pecah telur di tempat sebelah tarikan bersih bgt'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2606)\n",
      "\n",
      "üí¨ Input: 'Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.'\n",
      "üßπ Clean: 'udah masuk aja ke yang kemarin aku bilang yang tiap hari bagi hadiah itu'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1726)\n",
      "\n",
      "üí¨ Input: 'Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°'\n",
      "üßπ Clean: 'wkwk mantap jp lagi di platform terpercaya itu katanya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4236)\n",
      "\n",
      "üí¨ Input: 'bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.'\n",
      "üßπ Clean: 'bos ku sudah datang tadi pagi bawa hadiah 25k gila'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3169)\n",
      "\n",
      "üí¨ Input: 'Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.'\n",
      "üßπ Clean: 'ingat keberanian itu modal kalau mau hasil besar tempat itu udah aku rekomendasiin kemarin'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4233)\n",
      "\n",
      "üí¨ Input: 'Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!'\n",
      "üßπ Clean: 'semoga sukses selalu dan sehat terus ya kontennya sangat bermanfaat'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4994)\n",
      "\n",
      "üí¨ Input: 'Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.'\n",
      "üßπ Clean: 'saya transfer 500 ribu ke rekening teman tadi pagi semoga lancar'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4433)\n",
      "\n",
      "üí¨ Input: 'Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.'\n",
      "üßπ Clean: 'main mobile legends seru banget tapi harus top up dulu buat beli diamond'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4518)\n",
      "\n",
      "üí¨ Input: 'Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.'\n",
      "üßπ Clean: 'saya pesan nasi padang di warung bossku tadi rasanya mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2149)\n",
      "\n",
      "üí¨ Input: 'Saya sudah JP (Jelas Puas) sama pelayanan toko ini.'\n",
      "üßπ Clean: 'saya sudah jp jelas puas sama pelayanan toko ini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5520)\n",
      "\n",
      "üí¨ Input: 'Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.'\n",
      "üßπ Clean: 'komentar ini hanya berisi pujian tidak ada unsur promosi atau judi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2742)\n",
      "\n",
      "üí¨ Input: 'link untuk download materi kuliah ada di deskripsi video ya teman-teman.'\n",
      "üßπ Clean: 'link untuk download materi kuliah ada di deskripsi video ya teman teman'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2333)\n",
      "\n",
      "üí¨ Input: 'Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.'\n",
      "üßπ Clean: 'terima kasih atas video tutorial cara wd withdrawal uang dari platform saham'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6925)\n",
      "\n",
      "üí¨ Input: 'Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè'\n",
      "üßπ Clean: 'dijamin happy udah kaya rumah kedua arena923 disini'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2130)\n",
      "\n",
      "üí¨ Input: 'cuma di tempat lu bang yang paling amanahüôå'\n",
      "üßπ Clean: 'cuma di tempat lu bang yang paling amanah'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1210)\n",
      "\n",
      "üí¨ Input: 'ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya'\n",
      "üßπ Clean: 'megabet189 emang the real mvp selalu bikin kita bahagia dengan kemenangannya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2564)\n",
      "\n",
      "üí¨ Input: 'Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.'\n",
      "üßπ Clean: 'tadi pagi aku wd withdraw uang tunai dari atm 500 ribu'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6052)\n",
      "\n",
      "üí¨ Input: 'WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.'\n",
      "üßπ Clean: 'wd dana beasiswa sudah cair total 5 juta alhamdulillah'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.8831)\n",
      "\n",
      "üí¨ Input: 'Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.'\n",
      "üßπ Clean: 'jangan lupa wd hasil penjualan dari platform e commerce itu ya'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5996)\n",
      "\n",
      "üí¨ Input: 'Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!'\n",
      "üßπ Clean: 'pelayanan toko ini jp jelas puas banget barangnya gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5000)\n",
      "\n",
      "üí¨ Input: 'Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.'\n",
      "üßπ Clean: 'aku jp jaminan produk 100 kalau kamu beli di bossku official store'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.2785)\n",
      "\n",
      "üí¨ Input: 'Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!'\n",
      "üßπ Clean: 'wah si bossku tadi pagi datang bawa hadiah mantap jp'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.4891)\n",
      "\n",
      "üí¨ Input: 'Motor baruku Gacor banget, tarikannya mantap!'\n",
      "üßπ Clean: 'motor baruku gacor banget tarikannya mantap'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.1483)\n",
      "\n",
      "üí¨ Input: 'Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.'\n",
      "üßπ Clean: 'si bossku panggilan teman tadi depo deposit makanan ke meja'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5816)\n",
      "\n",
      "üí¨ Input: 'Restoran ini Gacor tiap hari, ramenya nggak ketulungan.'\n",
      "üßπ Clean: 'restoran ini gacor tiap hari ramenya nggak ketulungan'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.3495)\n",
      "\n",
      "üí¨ Input: 'Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).'\n",
      "üßπ Clean: 'aku dapat promo depo 50k dapat 50k di toko x bukan situs judi'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.6081)\n",
      "\n",
      "üí¨ Input: 'Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).'\n",
      "üßπ Clean: 'gacor banget main game mobile legends dapat jp juara pertama'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6869)\n",
      "\n",
      "üí¨ Input: 'Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).'\n",
      "üßπ Clean: 'kata bossku ini adalah situs terbaik untuk beli akun game bukan judi'\n",
      "üéØ Prediksi: üî• **JUDOL (SMOTE ML)** (Probabilitas: 0.6504)\n",
      "\n",
      "üí¨ Input: 'WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.'\n",
      "üßπ Clean: 'wd 5 ribu aja buat beli kopi soalnya dompet udah gacor'\n",
      "üéØ Prediksi: ‚úÖ Bukan Judol (SMOTE ML) (Probabilitas: 0.5541)\n",
      "\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI PENGUJIAN\n",
    "# ===============================================\n",
    "MODEL_PATH = \"judol_detection_augmented_smote.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer_augmented.pickle\"\n",
    "MAX_LEN = 100             # Harus sama dengan MAX_LEN saat training\n",
    "OPTIMAL_THRESHOLD = 0.65  # Threshold optimal yang Anda temukan\n",
    "\n",
    "# ===============================================\n",
    "# 1. DEFINISI FUNGSI CUSTOM (Focal Loss)\n",
    "# ===============================================\n",
    "# Focal Loss HARUS didefinisikan ulang agar Keras bisa memuat model\n",
    "def focal_loss(gamma=2., alpha=0.55): # Alpha 0.8 harus sama dengan saat training\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# ===============================================\n",
    "# 2. FUNGSI PREPROCESSING UNTUK PREDIKSI (Sesuai Training)\n",
    "# ===============================================\n",
    "def clean_text_for_prediction(text):\n",
    "    \"\"\"Fungsi cleaning yang digunakan saat prediksi (dengan normalisasi Unicode yang lebih robust)\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    # 1. Hapus URL\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    # NFKD: Mengubah karakter fancy (misal ùóõùó¢ùóßùêâùêÑùêèùêÑùü±ùü±) menjadi bentuk dasarnya (HOTJEPE55)\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Encode/Decode: Menghapus semua karakter non-ASCII (seperti emoji üÄÑü•∞, diakritik)\n",
    "    # yang tersisa setelah NFKD, hanya menyisakan karakter dasar ASCII.\n",
    "    # Ini adalah langkah kunci untuk mengatasi masalah hilangnya karakter fancy.\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 3. Lowercase (Dapat dilakukan setelah konversi ASCII)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 4. Hapus sisa karakter yang bukan a-z, 0-9, atau spasi\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # 5. Rapikan spasi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# ===============================================\n",
    "# 3. FUNGSI UTAMA PREDIKSI\n",
    "# ===============================================\n",
    "def predict_judol(text, model, tokenizer, threshold=OPTIMAL_THRESHOLD):\n",
    "    \"\"\"Memproses teks dan mengembalikan probabilitas dan label prediksi Judol.\"\"\"\n",
    "    \n",
    "    text_clean = clean_text_for_prediction(text)\n",
    "    \n",
    "    # 3.1 Tokenisasi dan Padding\n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    # 3.2 Prediksi Model\n",
    "    # model.predict mengembalikan array 2D [[probabilitas]]\n",
    "    # Matikan verbose agar output bersih\n",
    "    pred_prob = model.predict(pad, verbose=0)[0][0]\n",
    "    is_judol = pred_prob > threshold\n",
    "    \n",
    "    label = \"üî• **JUDOL (SMOTE ML)**\" if is_judol else \"‚úÖ Bukan Judol (SMOTE ML)\"\n",
    "    \n",
    "    return label, pred_prob, text_clean\n",
    "\n",
    "# ===============================================\n",
    "# 4. MEMUAT MODEL DAN TOKENIZER\n",
    "# ===============================================\n",
    "try:\n",
    "    # Memuat Model\n",
    "    print(f\"--- Memuat Model dari {MODEL_PATH} ---\")\n",
    "    model_loaded = load_model(MODEL_PATH, custom_objects={'loss': focal_loss(alpha=0.8)})\n",
    "    \n",
    "    # Memuat Tokenizer\n",
    "    print(f\"--- Memuat Tokenizer dari {TOKENIZER_PATH} ---\")\n",
    "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "        tokenizer_loaded = pickle.load(handle)\n",
    "        \n",
    "    print(\"\\n‚úÖ Model dan Tokenizer berhasil dimuat.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: File model atau tokenizer tidak ditemukan. Pastikan Anda sudah menjalankan script training.\")\n",
    "    exit()\n",
    "\n",
    "# ===============================================\n",
    "# 5. UJI COBA REAL-TIME\n",
    "# ===============================================\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"üöÄ UJI COBA MODEL DETEKSI JUDOL REAL-TIME (Threshold={OPTIMAL_THRESHOLD})\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "test_comments = [\n",
    "    # --- JUDOL TESTS ---\n",
    "    \"WD 500rb di GACOR777 mantap langsung cair!\",\n",
    "    \"buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!\",\n",
    "    \"main slot pasti untung, depo terus bosku\",\n",
    "    \"BOSKU WD 5 JUTA HARI INI DI SITUS RAJA88.\",\n",
    "    \"S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.\", \n",
    "    \"Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.\",\n",
    "    \"Lagi gacor main di maxwin888 hari ini.\",\n",
    "    \"Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞\", # Uji Kasus Perbaikan\n",
    "    \"WD 1O jt di BOSSku777. *lgsg cair*!\", \n",
    "    \"Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!\",\n",
    "    \"Aku di MANTAP888. Gacor terus!\", \n",
    "    \"buruan daftar di MANTAPJEPE888 promonya gede banget\", \n",
    "    \"Paling the best! HOTJEPE55 Memang GOOD JOB banget!\",\n",
    "    \"main di slotvip4d aja, depo 50 wd 500\", \n",
    "    \"gacorwd77 adalah situs paling oke.\",\n",
    "    \"wd 1000 jp. link ada di bio ya.\", \n",
    "    \"jp 2OOm jt sudah di WD. Paling gacor di sini.\", \n",
    "    \"S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok\",\n",
    "    \"asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.\", \n",
    "    \"tokowin303 4w1 likeReply\", \n",
    "    \"Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta\", \n",
    "    \"ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan\", \n",
    "    \"slotvip777 depo 50 wd 500\", \n",
    "    \"JP boss 44D mantap\",\n",
    "    \"üí∞ WD 5üü¢0üü¢0k di PULAUWIN 88! Langsung ü§ë JP!\", \n",
    "    \"S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat\", \n",
    "    \"ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j\", \n",
    "    \"W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!\", \n",
    "    \"jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!\", \n",
    "    \"p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!\", \n",
    "    \"garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar\", \n",
    "    \"wd sepuluh juta di slotvip111 üéØ langsung cair\", \n",
    "    \"wd cepat + JP 200k main di RAJASLOT777 sekarang\", \n",
    "    \"Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.\",\n",
    "    \"mAin di bùöòùöúÔªúùöû7ùüΩùüΩ enak bet, wd aman.\",\n",
    "    \"bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.\",\n",
    "    \"m‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.\",\n",
    "    \"Coba deh di \\\"QlRTT1QxMDA=\\\" katanya jam 3 pagi gacor.\",\n",
    "    \"Main di cptlv888 lumayan tarikannya.\",\n",
    "    \"Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.\",\n",
    "    \"b0sku77? gas bet wd ngebut bgt.\",\n",
    "    \"b o s k u\\n7 7 7\\n...disusun vertikal.\",\n",
    "    \"WD 600 tadi, no cap. That site hits diff fr.\",\n",
    "    \"M@!N di B0$KU^777 cuy wd kuzz teroosss.\",\n",
    "    \"bo sku 7 7 7 ‚Äî karena kamera burem jd gini.\",\n",
    "\n",
    "    # --- NON-JUDOL TESTS ---\n",
    "    \"Beli skin game baru ah, keren nih!\", \n",
    "    \"Ini channel YouTube favoritku, sering update video baru.\",\n",
    "    \"Makan malam apa hari ini?\",\n",
    "    \"Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan\",\n",
    "    \"WD 50 ribu saja, buat beli kopi.\", \n",
    "    \"wah mantap nih, motor baru udah di WD sama pemiliknya\", \n",
    "    \"itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.\",\n",
    "    \"Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.\", \n",
    "    \"Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.\",\n",
    "    \"Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°\",\n",
    "    \"bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.\",\n",
    "    \"Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.\",\n",
    "    \"Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!\", \n",
    "    \"Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.\", \n",
    "    \"Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.\", \n",
    "    \"Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.\", \n",
    "    \"Saya sudah JP (Jelas Puas) sama pelayanan toko ini.\", \n",
    "    \"Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.\",\n",
    "    \"link untuk download materi kuliah ada di deskripsi video ya teman-teman.\", \n",
    "    \"Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.\" ,\n",
    "    \"Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè\",\n",
    "    \"cuma di tempat lu bang yang paling amanahüôå\",\n",
    "    \"ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya\",\n",
    "    # Non-Judol menggunakan WD (Withdraw) dan Nominal\n",
    "    \"Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.\",\n",
    "    \"WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.\",\n",
    "    \"Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.\",\n",
    "    \n",
    "    # Non-Judol menggunakan JP (Jelas Puas, Jaminan Produk, etc.)\n",
    "    \"Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!\",\n",
    "    \"Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.\",\n",
    "    \"Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!\",\n",
    "\n",
    "    # Non-Judol menggunakan Gacor, Bossku, Depo (konteks non-judi)\n",
    "    \"Motor baruku Gacor banget, tarikannya mantap!\",\n",
    "    \"Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.\",\n",
    "    \"Restoran ini Gacor tiap hari, ramenya nggak ketulungan.\",\n",
    "    \n",
    "    # Non-Judol dengan Kombinasi Kata Kunci Ambigu\n",
    "    \"Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).\",\n",
    "    \"Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).\",\n",
    "    \"Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).\",\n",
    "    \"WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.\" # Gacor = Kosong\n",
    "]\n",
    "\n",
    "for comment in test_comments:\n",
    "    # Memastikan model tidak menampilkan pesan saat memprediksi\n",
    "    label, prob, clean_text = predict_judol(comment, model_loaded, tokenizer_loaded, OPTIMAL_THRESHOLD)\n",
    "    \n",
    "    print(f\"\\nüí¨ Input: '{comment}'\")\n",
    "    print(f\"üßπ Clean: '{clean_text}'\")\n",
    "    print(f\"üéØ Prediksi: {label} (Probabilitas: {prob:.4f})\")\n",
    "\n",
    "print(\"\\n=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f74beb-fa8c-4dd1-8ddf-44b06788f0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# --- MEMBUTUHKAN SASTRAWI UNTUK KONSISTENSI PREPROCESSING ---\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI PENGUJIAN\n",
    "# ===============================================\n",
    "MODEL_PATH = \"judol_detection_augmented_smote.h5\"\n",
    "TOKENIZER_PATH = \"tokenizer_augmented.pickle\"\n",
    "MAX_LEN = 100             # Harus sama dengan MAX_LEN saat training\n",
    "OPTIMAL_THRESHOLD = 0.65  # Kembali ke threshold optimal awal (atau disesuaikan nanti)\n",
    "TRAINING_ALPHA = 0.55     # Alpha Focal Loss dari training\n",
    "\n",
    "# ===============================================\n",
    "# 0. INISIALISASI SASTRAWI\n",
    "# ===============================================\n",
    "# Inisialisasi Sastrawi harus SAMA persis dengan yang digunakan saat training\n",
    "try:\n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopword_remover = factory.create_stop_word_remover()\n",
    "\n",
    "    factory_stemmer = StemmerFactory()\n",
    "    stemmer = factory_stemmer.create_stemmer()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Gagal inisialisasi Sastrawi. Pastikan Sastrawi sudah terinstall.\")\n",
    "    print(f\"Detail error: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 1. DEFINISI FUNGSI CUSTOM (Focal Loss)\n",
    "# ===============================================\n",
    "def focal_loss(gamma=2., alpha=TRAINING_ALPHA): \n",
    "    \"\"\"Fungsi Focal Loss yang dibutuhkan untuk memuat model Keras.\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# ===============================================\n",
    "# 2. FUNGSI PREPROCESSING KONSISTEN (Menggabungkan semua langkah training)\n",
    "# ===============================================\n",
    "def clean_text_for_prediction(text, stopword_remover, stemmer):\n",
    "    \"\"\"\n",
    "    Fungsi cleaning yang MENCERMINKAN SECARA TEPAT langkah-langkah dalam text preprocessing.py\n",
    "    (tanpa fungsi remove_text_decorations yang tidak diberikan, diasumsikan sudah ada di langkah awal).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Normalisasi Unicode (NFKD) & Konversi ke ASCII (Langkah dari skrip lama, tetap dipertahankan)\n",
    "    # Ini membantu mengatasi font-font unik seperti pada HOTJEPE55\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 2. Hapus URL (Langkah dari skrip lama, tetap dipertahankan)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    # 3. Lowercase (DILAKUKAN SEBELUM STOPWORD/STEMMING)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 4. Hapus karakter yang bukan a-z, 0-9, atau spasi (sesuai langkah 7 di training)\n",
    "    # Di sini kita hanya menyisakan alfanumerik dan spasi agar konsisten sebelum Sastrawi\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # 5. Rapikan spasi dan trim (Langkah dari skrip lama, tetap dipertahankan)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # --- LANGKAH PENTING: STOPWORD REMOVAL (Sastrawi) ---\n",
    "    text = stopword_remover.remove(text)\n",
    "    \n",
    "    # --- LANGKAH PENTING: STEMMING (Sastrawi) ---\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    # 6. Rapikan spasi lagi setelah stemming/stopword\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ===============================================\n",
    "# 3. FUNGSI UTAMA PREDIKSI\n",
    "# ===============================================\n",
    "def predict_judol(text, model, tokenizer, threshold, stopword_remover, stemmer):\n",
    "    \"\"\"Memproses teks dan mengembalikan probabilitas dan label prediksi Judol.\"\"\"\n",
    "    \n",
    "    # Meneruskan objek Sastrawi ke fungsi cleaning\n",
    "    text_clean = clean_text_for_prediction(text, stopword_remover, stemmer)\n",
    "    \n",
    "    # 3.1 Tokenisasi dan Padding\n",
    "    seq = tokenizer.texts_to_sequences([text_clean])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    \n",
    "    # 3.2 Prediksi Model\n",
    "    if pad.size == 0:\n",
    "        pred_prob = 0.0\n",
    "    else:\n",
    "        pred_prob = model.predict(pad, verbose=0)[0][0]\n",
    "        \n",
    "    is_judol = pred_prob > threshold\n",
    "    \n",
    "    label = f\"üî• **JUDOL (SMOTE ML)** (Prob > {threshold})\" if is_judol else f\"‚úÖ Bukan Judol (Prob <= {threshold})\"\n",
    "    \n",
    "    return label, pred_prob, text_clean\n",
    "\n",
    "# ===============================================\n",
    "# 4. MEMUAT MODEL DAN TOKENIZER\n",
    "# ===============================================\n",
    "try:\n",
    "    print(f\"--- Memuat Model dari {MODEL_PATH} (Alpha={TRAINING_ALPHA}) ---\")\n",
    "    model_loaded = load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects={'loss': focal_loss(alpha=TRAINING_ALPHA)} \n",
    "    )\n",
    "    \n",
    "    print(f\"--- Memuat Tokenizer dari {TOKENIZER_PATH} ---\")\n",
    "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "        tokenizer_loaded = pickle.load(handle)\n",
    "        \n",
    "    print(\"\\n‚úÖ Model dan Tokenizer berhasil dimuat.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File model ({MODEL_PATH}) atau tokenizer ({TOKENIZER_PATH}) tidak ditemukan.\")\n",
    "    sys.exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Gagal memuat model atau tokenizer: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# ===============================================\n",
    "# 5. UJI COBA REAL-TIME DATASET\n",
    "# ===============================================\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"üöÄ UJI COBA MODEL DETEKSI JUDOL KONSISTEN (Threshold={OPTIMAL_THRESHOLD})\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "test_comments = [\n",
    "    # --- JUDOL TESTS (Target: TP) ---\n",
    "    \"WD 500rb di GACOR777 mantap langsung cair!\", # TP\n",
    "    \"buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!\", # TP\n",
    "    \"jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!\", # TP\n",
    "    \"WD 1O jt di BOSSku777. *lgsg cair*!\", # TP\n",
    "    \"Tokowin303 4w1 likeReply\", # TP\n",
    "    \n",
    "    # --- NON-JUDOL TESTS (Target: TN - Fokus Perbaikan FP) ---\n",
    "    \"Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!\", # FP sebelumnya 0.9704 -> Target: TN\n",
    "    \"Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.\", # FP sebelumnya 0.9724 -> Target: TN\n",
    "    \"WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.\", # FP sebelumnya 0.9466 -> Target: TN\n",
    "    \"Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.\", # FP sebelumnya 0.9527 -> Target: TN\n",
    "    \"Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!\", # FP sebelumnya 0.9426 -> Target: TN\n",
    "    \"Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).\", # TN\n",
    "    \"Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.\", # TN\n",
    "    \"WD 50 ribu saja, buat beli kopi.\" # TN\n",
    "]\n",
    "\n",
    "for comment in test_comments:\n",
    "    # Menggunakan fungsi prediksi yang konsisten\n",
    "    label, prob, clean_text = predict_judol(comment, model_loaded, tokenizer_loaded, OPTIMAL_THRESHOLD, stopword_remover, stemmer)\n",
    "    \n",
    "    print(f\"\\nüí¨ Input: '{comment}'\")\n",
    "    print(f\"üßπ Clean: '{clean_text}'\")\n",
    "    print(f\"üéØ Prediksi: {label} (Probabilitas: {prob:.4f})\")\n",
    "\n",
    "print(\"\\n=============================================\")\n",
    "print(\"Uji coba selesai.\")\n",
    "print(\"Perhatikan kolom 'Clean' sekarang. Teks harusnya lebih pendek karena stopword sudah hilang dan kata-kata sudah di-stem.\")\n",
    "print(\"Jika False Positive (FP) yang confidence-nya tinggi (seperti 'Semoga sukses...') kini menjadi True Negative (TN), berarti konsistensi preprocessing adalah solusinya.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71dcc1-0776-4ea5-bfb4-2b88af7d1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- MODUL WAJIB ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "# Modul Sastrawi Wajib\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "# Modul Tambahan untuk Progress Bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI UTAMA & INISIALISASI SASTRAWI\n",
    "# ===============================================\n",
    "FILE_PATH = \"final_production_judol_detection.csv\"\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "NEW_ALPHA = 0.55\n",
    "OPTIMAL_THRESHOLD = 0.65\n",
    "TEXT_COLUMN = 'cleaned_comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "TEST_SIZE_FINAL = 0.20\n",
    "NEW_MODEL_AUG_PATH = \"judol_detection_augmented_smote.keras\" # <--- PERUBAHAN UTAMA: Gunakan ekstensi .keras\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# INISIALISASI SASTRAWI\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword_remover = factory.create_stop_word_remover()\n",
    "factory_stemmer = StemmerFactory()\n",
    "stemmer = factory_stemmer.create_stemmer()\n",
    "\n",
    "# ===============================================\n",
    "# 1. DATA AUGMENTASI: SINTESIS JUDOL (LABEL 1)\n",
    "# ===============================================\n",
    "prefixes = ['slot', 'jp', 'wd', 'depo', 'gacor', 'raja', 'bosku', 'mantap']\n",
    "numbers = [str(i) for i in range(100, 999, 100)] + [str(i) for i in range(11, 99, 11)]\n",
    "phrases = ['gacor banget di sini!', 'pasti jp di situs ini', 'wd cepat tanpa ribet', 'cair terus bosku']\n",
    "\n",
    "synthetic_data_pos = []\n",
    "for i in range(200):\n",
    "    prefix = np.random.choice(prefixes)\n",
    "    num = np.random.choice(numbers)\n",
    "    phrase = np.random.choice(phrases)\n",
    "    brand_name = prefix + num\n",
    "    comment = f\"{phrase} coba main di {brand_name} aja deh\"\n",
    "    synthetic_data_pos.append({\"comment_text\": comment, \"target\": 1})\n",
    "\n",
    "df_synthetic_pos = pd.DataFrame(synthetic_data_pos)\n",
    "\n",
    "# ===============================================\n",
    "# 2. DATA AUGMENTASI: SINTESIS NON-JUDOL (LABEL 0)\n",
    "# ===============================================\n",
    "NEGATIVE_AUGMENTATION_DATA = [\n",
    "    \"WD uang beasiswa sudah cair 5 juta alhamdulillah\",\n",
    "    \"WD dana dari platform saham tutorialnya jelas\",\n",
    "    \"WD gaji dari kantor total 10 juta\",\n",
    "    \"WD uang tunai 500 ribu dari ATM\",\n",
    "    \"Withdrawal dana P2P lending cair ke rekening\",\n",
    "    \"tarik uang 1 juta untuk beli buku kuliah\",\n",
    "    \"gacor banget motor tarikan mantap\",\n",
    "    \"gacor banget pelayanan toko ini\",\n",
    "    \"jp jelas puas sama produknya\",\n",
    "    \"motor baru ini gacor sekali\",\n",
    "    \"mobil jerman ini tarikannya mantap\",\n",
    "    \"jp (jelas puas) sama hasilnya\",\n",
    "    \"bosku beli kopi di warung\",\n",
    "    \"saya sudah deposit uang di bank\",\n",
    "    \"depo uang kuliah semester 3\",\n",
    "    \"akun ini sudah top up diamond\",\n",
    "    \"kami adalah agen perjalanan resmi\",\n",
    "]\n",
    "\n",
    "df_neg_aug = pd.DataFrame({'comment_text': NEGATIVE_AUGMENTATION_DATA, 'target': 0})\n",
    "\n",
    "print(f\"‚úÖ {len(df_synthetic_pos)} data Judol sintetik ditambahkan.\")\n",
    "print(f\"‚úÖ {len(df_neg_aug)} data Non-Judol (negatif) augmentasi ditambahkan.\")\n",
    "\n",
    "# ===============================================\n",
    "# 3. FUNGSI PREPROCESSING KONSISTEN\n",
    "# (Mencakup Normalisasi Samaran, Stopword, dan Stemming)\n",
    "# ===============================================\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning konsisten dengan Stemming dan Normalisasi Samaran.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Normalisasi Unicode & Konversi ke ASCII\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 2. Lowercase, Hapus URL/Mention\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+|@\\S+\", \"\", text)\n",
    "    \n",
    "    # 3. Hapus karakter yang bukan a-z, 0-9, atau spasi\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # --- NORMALISASI SAMARAN AGRESIF ---\n",
    "    text_parts = text.split()\n",
    "    processed_parts = []\n",
    "    i = 0\n",
    "    while i < len(text_parts):\n",
    "        current_part = text_parts[i]\n",
    "        j = i + 1\n",
    "        merged_word = current_part\n",
    "        while j < len(text_parts) and len(text_parts[j]) <= 1 and re.match(r'[a-z0-9]', text_parts[j]):\n",
    "            merged_word += text_parts[j]\n",
    "            j += 1\n",
    "        \n",
    "        if len(merged_word) > len(current_part):\n",
    "            processed_parts.append(merged_word)\n",
    "            i = j\n",
    "        else:\n",
    "            processed_parts.append(current_part)\n",
    "            i += 1\n",
    "\n",
    "    text = \" \".join(processed_parts)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # 4. Rapikan spasi dan trim\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 5. STOPWORD REMOVAL (Sastrawi)\n",
    "    text = stopword_remover.remove(text)\n",
    "    \n",
    "    # 6. STEMMING (Sastrawi)\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    # 7. Rapikan spasi lagi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 4. LOAD & GABUNGKAN DATA\n",
    "# ===============================================\n",
    "print(\"\\n--- 3. Memuat Data, Menggabungkan Augmentasi, dan Preprocessing ---\")\n",
    "try:\n",
    "    df_original = pd.read_csv(FILE_PATH)\n",
    "    df_original = df_original.dropna(subset=['comment_text', TARGET_COLUMN])\n",
    "    df_original[TARGET_COLUMN] = df_original[TARGET_COLUMN].astype(int)\n",
    "    \n",
    "    df_combined = pd.concat([\n",
    "        df_original[['comment_text', TARGET_COLUMN]], \n",
    "        df_synthetic_pos,\n",
    "        df_neg_aug\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    print(\"Menerapkan cleaning, stopword removal, dan stemming...\")\n",
    "    df_combined[TEXT_COLUMN] = df_combined['comment_text'].fillna('').progress_apply(clean_text_prep)\n",
    "    \n",
    "    df_combined = df_combined[df_combined[TEXT_COLUMN].str.len() > 0]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File '{FILE_PATH}' tidak ditemukan. Pastikan file berada di direktori yang sama.\")\n",
    "    sys.exit()\n",
    "\n",
    "X = df_combined[TEXT_COLUMN]\n",
    "Y = df_combined[TARGET_COLUMN]\n",
    "\n",
    "print(f\"\\nTotal data akhir setelah semua augmentasi dan cleaning: {len(X)} baris.\")\n",
    "print(f\"Jumlah Judol (1) sebelum split: {Y.sum()}\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 5. SPLIT DATA (STRATIFIED)\n",
    "# ===============================================\n",
    "print(\"\\n--- 4. Split Data (80% Train, 20% Test Stratified) ---\")\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    X, \n",
    "    Y, \n",
    "    test_size=TEST_SIZE_FINAL, \n",
    "    random_state=SEED, \n",
    "    stratify=Y \n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# 6. FEATURE EXTRACTION (Tokenisasi & Padding)\n",
    "# ===============================================\n",
    "print(\"\\n--- 5. Feature Extraction: Tokenisasi dan Padding ---\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text.astype(str))\n",
    "\n",
    "TOKENIZER_PATH = 'tokenizer_augmented.pickle'\n",
    "with open(TOKENIZER_PATH, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"‚úÖ Tokenizer disimpan sebagai '{TOKENIZER_PATH}'\")\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text.astype(str))\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text.astype(str))\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 7. TERAPKAN SMOTE (HANYA PADA DATA LATIH)\n",
    "# ===============================================\n",
    "print(\"\\n--- 6. Terapkan SMOTE (Oversampling pada Data Latih) ---\")\n",
    "Y_train_np = Y_train.values \n",
    "smote = SMOTE(sampling_strategy='minority', random_state=SEED)\n",
    "\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_train_padded, Y_train_np)\n",
    "\n",
    "X_train_final = np.array(X_resampled)\n",
    "Y_train_final = np.array(Y_resampled)\n",
    "\n",
    "judol_count = Y_train_final.sum()\n",
    "non_judol_count = len(Y_train_final) - judol_count\n",
    "print(f\"‚úÖ Data Latih Akhir Setelah SMOTE: {len(X_train_final)} baris (Rasio 1:1)\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 8. TRAINING DENGAN DATA SMOTE DAN FOCAL LOSS\n",
    "# ===============================================\n",
    "# Definisi Focal Loss harus dipertahankan karena digunakan di compile()\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "    \n",
    "model_aug = Sequential([\n",
    "    Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# NOTE: Saat meng-compile, kita masih menggunakan custom loss\n",
    "model_aug.compile(loss=focal_loss(), optimizer=Adam(learning_rate=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "]\n",
    "\n",
    "print(f\"\\n--- 7. Memulai Pelatihan Model (Alpha={NEW_ALPHA}) ---\")\n",
    "\n",
    "history = model_aug.fit(\n",
    "    X_train_final, Y_train_final, \n",
    "    validation_split=0.1, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# PERUBAHAN KRITIS: Hanya panggil save() dengan path .keras dan overwrite=True\n",
    "model_aug.save(\n",
    "    NEW_MODEL_AUG_PATH, \n",
    "    overwrite=True, \n",
    ")\n",
    "print(f\"\\nModel baru disimpan sebagai '{NEW_MODEL_AUG_PATH}' ‚úÖ\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 9. EVALUASI DAN UJI MODEL BARU\n",
    "# ===============================================\n",
    "\n",
    "print(\"\\n=============================================\")\n",
    "print(\"üìä EVALUASI AKHIR MODEL PADA DATA UJI MURNI\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "Y_test_np = Y_test.values \n",
    "\n",
    "# NOTE: Jika Anda perlu memuat model di skrip lain, Anda harus menggunakan custom_objects di load_model.\n",
    "# Contoh: loaded_model = load_model(NEW_MODEL_AUG_PATH, custom_objects={'loss': focal_loss(alpha=NEW_ALPHA)})\n",
    "\n",
    "loss, accuracy = model_aug.evaluate(X_test_padded, Y_test_np, verbose=0) \n",
    "\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")\n",
    "print(f\"Akurasi pada Data Uji: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_proba = model_aug.predict(X_test_padded, verbose=0)\n",
    "y_pred_classes = (y_pred_proba > OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\nLaporan Klasifikasi (Threshold={OPTIMAL_THRESHOLD}):\")\n",
    "print(classification_report(Y_test_np, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe114c0-60f7-442e-a40d-ad25ae6e26cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# PENTING: Pustaka Sastrawi diimpor tetapi fungsi Stemming dan Stopword TIDAK DIGUNAKAN.\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory \n",
    "# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# PENTING: Anda harus menginstal TensorFlow untuk menjalankan kode ini\n",
    "try:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import keras\n",
    "except ImportError:\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "    print(\"PERINGATAN: Pustaka 'tensorflow' tidak ditemukan. Pastikan sudah terinstal.\")\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "\n",
    "# --- CUSTOM OBJECTS FIX (Diperlukan untuk memuat model dengan custom loss) ---\n",
    "@keras.saving.register_keras_serializable()\n",
    "def loss(y_true, y_pred):\n",
    "    \"\"\"Fungsi placeholder untuk custom loss.\"\"\"\n",
    "    return keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "# --- KONSTANTA & PERUBAHAN ---\n",
    "MAX_LEN = 50\n",
    "# Ambang batas tetap 0.6, tetapi kita akan menambahkan deteksi nama situs yang kuat.\n",
    "THRESHOLD = 0.6 \n",
    "MODEL_PATH = 'judol_detection_augmented_smote.keras'\n",
    "TOKENIZER_PATH = 'tokenizer_augmented.pickle'\n",
    "\n",
    "# Perbaikan A: Daftar nama situs yang wajib ditandai sebagai JUDOL\n",
    "HARD_JUDOL_NAMES = [\n",
    "    'hotjepe55', 'arena923', 'megabet189', 'labag91', \n",
    "    'tokowin303', 'gacorwd77', 'mantul404', 'galabet688',\n",
    "    'slotvip777', 'maxwin888', 'bossku777', 'rajaslot777', \n",
    "    'mantapjepe888', 'slotvip4d', 'mantap888', 'pulauwin88', \n",
    "    'slotvip111', 'garudahoki', 'sg188', 'cptlv888'\n",
    "]\n",
    "\n",
    "# --- FUNGSI PRA-PEMROSESAN YANG DITINGKATKAN (FIXED V2) ---\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"\n",
    "    Melakukan pra-pemrosesan yang agresif pada samaran/obfuscation, \n",
    "    MEMPERTAHANKAN konteks, dan MENANDAI nama-nama situs judi (HARD-JUDOL).\n",
    "    \"\"\"\n",
    "    # 1. Unicode Normalization (Mengatasi Fancy Fonts)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # 2. Case Folding (Mengubah ke huruf kecil)\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3. Membersihkan Samaran & Simbol\n",
    "    text = re.sub(r'[\\@\\$\\!\\#\\^\\&\\*\\(\\)\\-=\\+\\|\\\\\\[\\]\\{\\}\\;\\:\\'\\\"\\<\\,\\>\\.\\/\\?]', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text) \n",
    "\n",
    "    # 4. Konsolidasi spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 5. Perbaikan A: Menandai Nama Situs Keras (HARD JUDOL)\n",
    "    # Ini harus dilakukan setelah normalisasi dan pembersihan simbol/spasi.\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Cek apakah kata tersebut (atau varian dekatnya) adalah nama situs di daftar HARD_JUDOL_NAMES\n",
    "        is_hard_judol = False\n",
    "        for site_name in HARD_JUDOL_NAMES:\n",
    "            # Gunakan regex untuk mencocokkan kata, mengabaikan angka yang dipecah (misal 9 2 3)\n",
    "            if re.search(site_name, word):\n",
    "                is_hard_judol = True\n",
    "                break\n",
    "        \n",
    "        if is_hard_judol:\n",
    "            # Ganti nama situs terdeteksi dengan token 'SITUSJUDOL'\n",
    "            processed_words.append('SITUSJUDOL')\n",
    "        else:\n",
    "            processed_words.append(word)\n",
    "\n",
    "    # Gabungkan kembali menjadi teks\n",
    "    text = ' '.join(processed_words)\n",
    "\n",
    "    # Finalisasi spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# --- FUNGSI UTAMA DETEKSI ---\n",
    "def predict_judol(texts, model, tokenizer, max_len=MAX_LEN, threshold=THRESHOLD):\n",
    "    \"\"\"\n",
    "    Melakukan pra-pemrosesan, tokenisasi, prediksi model, dan menentukan label.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(texts, columns=['Teks Asli'])\n",
    "    \n",
    "    # 1. Pra-pemrosesan (Termasuk penandaan nama situs)\n",
    "    df['Teks Bersih'] = df['Teks Asli'].apply(clean_text_prep)\n",
    "    \n",
    "    # 2. Tokenisasi dan Padding\n",
    "    sequences = tokenizer.texts_to_sequences(df['Teks Bersih'].tolist())\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    # 3. Prediksi\n",
    "    probabilities = model.predict(padded_sequences, verbose=0).flatten()\n",
    "    df['Prob. Judol'] = probabilities\n",
    "    \n",
    "    # 4. Pelabelan\n",
    "    df['Label'] = np.where(probabilities >= threshold, 'JUDOL', 'NON-JUDOL')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- FUNGSI UTAMA EKSEKUSI DEMO ---\n",
    "def main():\n",
    "    print(\"Mengambil model dari '{}' dan Tokenizer dari '{}'...\".format(MODEL_PATH, TOKENIZER_PATH))\n",
    "    \n",
    "    try:\n",
    "        # Tentukan objek kustom yang diperlukan untuk memuat model\n",
    "        custom_objects_map = {'loss': loss}\n",
    "        \n",
    "        # Pemuatan Model dan Tokenizer (Membutuhkan file yang ada)\n",
    "        model = load_model(MODEL_PATH, custom_objects=custom_objects_map)\n",
    "        with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "        \n",
    "        print(\"‚úÖ Model dan Tokenizer berhasil dimuat.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ERROR: File model atau tokenizer tidak ditemukan. Pastikan kedua file ada.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: Gagal memuat model atau tokenizer. Detail: {e}\")\n",
    "        return\n",
    "\n",
    "    # Data Uji Coba Batch (Dari prompt pengguna)\n",
    "    test_texts = [\n",
    "        \"Judi slot gacor hari ini pasti menang deposit 10k di link bio kami\", # JUDOL\n",
    "        \"Selamat siang semua, jangan lupa makan siang ya! Ini adalah cuaca yang cerah sekali.\", # NON-JUDOL\n",
    "        \"main di situs MPO444 sekarang dan langsung withdraw! dijamin jp\", # JUDOL\n",
    "        \"video ini sangat bagus dan mengedukasi sekali tentang sejarah peradaban kuno di mesir.\", # NON-JUDOL\n",
    "        \"Ayo join situs bola aman 11betting terpercaya di indonesia dan dapatkan bonus terbesar hari ini.\", # JUDOL\n",
    "        \"Kapan episode selanjutnya tayang? Menunggu banget nih. Kami ingin tahu kelanjutannya.\", # NON-JUDOL\n",
    "        \"link alternatif 77slot gampang menang nih bosku. klik link di bio.\", # JUDOL\n",
    "        \"Selamat ulang tahun untuk teman baikku! Semoga panjang umur dan sehat selalu ya.\", # NON-JUDOL\n",
    "        \"Ayo buruan daftar di MPO 7 77 sekarang, dapatkan bonus terbesar!\", # JUDOL\n",
    "        \"WD 500rb di GACOR777 mantap langsung cair!\", # JUDOL\n",
    "        \"buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!\", # JUDOL\n",
    "        \"main slot pasti untung, depo terus bosku\", # JUDOL\n",
    "        \"JP boss 44D mantap\", # NON-JUDOL\n",
    "        \"S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.\", # JUDOL\n",
    "        \"Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.\", # JUDOL\n",
    "        \"Lagi gacor main di maxwin888 hari ini. Lomba makan.\", # JUDOL (Target perbaikan 1)\n",
    "        \"Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞\", # JUDOL (Target perbaikan 2)\n",
    "        \"WD 1O jt di BOSSku777. *lgsg cair*!\", # JUDOL\n",
    "        \"Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!\", # JUDOL\n",
    "        \"Aku di MANTAP888. Gacor terus!\", # JUDOL (Target perbaikan 3)\n",
    "        \"buruan daftar di MANTAPJEPE888 promonya gede banget\", # JUDOL\n",
    "        \"Paling the best! HOTJEPE55 Memang GOOD JOB banget!\", # JUDOL (Target perbaikan 4)\n",
    "        \"main di slotvip4d aja, depo 50 wd 500\", # JUDOL\n",
    "        \"gacorwd77 adalah situs paling oke.\", # JUDOL (Target perbaikan 5)\n",
    "        \"wd 1000 jp. link ada di bio ya.\", # JUDOL\n",
    "        \"jp 2OOm jt sudah di WD. Paling gacor di sini.\", # JUDOL\n",
    "        \"S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok\", # JUDOL\n",
    "        \"asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.\", # JUDOL (Target perbaikan 6)\n",
    "        \"tokowin303 4w1 likeReply\", # JUDOL (Target perbaikan 7)\n",
    "        \"Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta\", # JUDOL (Target perbaikan 8)\n",
    "        \"ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan\", # JUDOL (Target perbaikan 9)\n",
    "        \"slotvip777 depo 50 wd 500\", # JUDOL\n",
    "        \"JP boss 44D mantap\", # NON-JUDOL\n",
    "        \"üí∞ WD 5üü¢0üü¢0k di P U L A U W I N 88! Langsung ü§ë JP! Lomba\", # JUDOL\n",
    "        \"S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat\", # JUDOL\n",
    "        \"ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j\", # JUDOL\n",
    "        \"W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!\", # JUDOL\n",
    "        \"jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!\", # JUDOL\n",
    "        \"p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!\", # JUDOL\n",
    "        \"garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar\", # JUDOL\n",
    "        \"wd sepuluh juta di slotvip111 üéØ langsung cair\", # JUDOL\n",
    "        \"wd cepat + JP 200k main di RAJASLOT777 sekarang\", # JUDOL\n",
    "        \"Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.\", # JUDOL\n",
    "        \"mAin di bùöòùöúùöîùöû7ùüΩùüΩ enak bet, wd aman.\", # JUDOL\n",
    "        \"bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.\", # JUDOL\n",
    "        \"m‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.\", # JUDOL\n",
    "        \"Coba deh di \\\"QlRTT1QxMDA=\\\" katanya jam 3 pagi gacor.\", # JUDOL\n",
    "        \"Main di cptlv888 lumayan tarikannya. Lomba\", # JUDOL (Target perbaikan 10)\n",
    "        \"Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.\", # NON-JUDOL\n",
    "        \"b0sku77? gas bet wd ngebut bgt.\", # JUDOL\n",
    "        \"b o s k u 7 7 7 ...disusun vertikal.\", # JUDOL\n",
    "        \"WD 600 tadi, no cap. That site hits diff fr.\", # JUDOL\n",
    "        \"M@!N di B0$KU^777 cuy wd kuzz teroosss.\", # JUDOL\n",
    "        \"bo sku 7 7 7 ‚Äî karena kamera burem jd gini.\", # JUDOL\n",
    "        \"Beli skin game baru ah, keren nih!\", # NON-JUDOL\n",
    "        \"Ini channel YouTube favoritku, sering update video baru.\", # NON-JUDOL\n",
    "        \"Makan malam apa hari ini?\", # NON-JUDOL\n",
    "        \"Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan\", # NON-JUDOL\n",
    "        \"WD 50 ribu saja, buat beli kopi.\", # NON-JUDOL\n",
    "        \"wah mantap nih, motor baru udah di WD sama pemiliknya\", # NON-JUDOL\n",
    "        \"itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.\", # JUDOL (Target perbaikan 11)\n",
    "        \"Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.\", # JUDOL\n",
    "        \"Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.\", # JUDOL\n",
    "        \"Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°\", # NON-JUDOL\n",
    "        \"bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.\", # NON-JUDOL\n",
    "        \"Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.\", # JUDOL\n",
    "        \"Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!\", # NON-JUDOL\n",
    "        \"Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.\", # NON-JUDOL\n",
    "        \"Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.\", # NON-JUDOL\n",
    "        \"Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.\", # NON-JUDOL\n",
    "        \"Saya sudah JP (Jelas Puas) sama pelayanan toko ini.\", # NON-JUDOL\n",
    "        \"Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.\", # NON-JUDOL\n",
    "        \"link untuk download materi kuliah ada di deskripsi video ya teman-teman.\", # NON-JUDOL\n",
    "        \"Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.\", # NON-JUDOL\n",
    "        \"Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè\", # JUDOL (Target perbaikan 12)\n",
    "        \"cuma di tempat lu bang yang paling amanahüôå\", # JUDOL\n",
    "        \"ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya\", # JUDOL (Target perbaikan 13)\n",
    "        \"Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.\", # NON-JUDOL\n",
    "        \"WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.\", # NON-JUDOL\n",
    "        \"Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.\", # NON-JUDOL\n",
    "        \"Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!\", # NON-JUDOL\n",
    "        \"Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.\", # NON-JUDOL\n",
    "        \"Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!\", # NON-JUDOL\n",
    "        \"Motor baruku Gacor banget, tarikannya mantap!\", # NON-JUDOL\n",
    "        \"Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.\", # NON-JUDOL\n",
    "        \"Restoran ini Gacor tiap hari, ramenya nggak ketulungan.\", # NON-JUDOL\n",
    "        \"Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).\", # NON-JUDOL\n",
    "        \"Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).\", # NON-JUDOL\n",
    "        \"Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).\", # NON-JUDOL\n",
    "        \"WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.\" # NON-JUDOL\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"DEMO PREDIKSI BATCH (Ambangan Batas/Threshold: {THRESHOLD})\")\n",
    "    print(f\"Model: {MODEL_PATH}\")\n",
    "    print(\"Pre-processing: FIXED V2 - Marking Hard Judol Names (e.g., arena923 -> SITUSJUDOL)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Jalankan Prediksi\n",
    "    results_df = predict_judol(test_texts, model, tokenizer)\n",
    "\n",
    "    # Menampilkan Hasil dengan Formatting Rapi\n",
    "    print(\"No.  | Prob. Judol  | Label        | Teks Asli\")\n",
    "    print(\"-----+--------------+--------------+-----------------------------------------------------------------------\")\n",
    "    \n",
    "    for i, row in results_df.iterrows():\n",
    "        # Teks Asli dan Teks Bersih\n",
    "        original_text = row['Teks Asli']\n",
    "        clean_text = row['Teks Bersih']\n",
    "\n",
    "        # Baris pertama (dengan Probabilitas dan Label)\n",
    "        print(f\"{i+1:<4} | {row['Prob. Judol']:<12.4f} | {row['Label']:<12} | {original_text:<70}\")\n",
    "\n",
    "        # Tambahkan baris untuk Teks Bersih\n",
    "        print(f\"     |              |              | (Bersih): {clean_text:<70}\")\n",
    "        \n",
    "        print(\"-\" * 87)\n",
    "    \n",
    "    print(\"\\n--- Keterangan ---\")\n",
    "    print(f\"Label JUDOL: Promosi Perjudian Online (Probabilitas >= {THRESHOLD})\")\n",
    "    print(f\"Label NON-JUDOL: Komentar Aman (Probabilitas < {THRESHOLD})\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc79b65-8447-443c-bca3-150f7043d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# --- MODUL WAJIB ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURASI UTAMA & INISIALISASI SASTRAWI\n",
    "# ===============================================\n",
    "FILE_PATH = \"final_production_judol_detection.csv\"\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "LR = 1e-3\n",
    "SEED = 42\n",
    "# <<< PERUBAHAN KRITIS UNTUK MENINGKATKAN PRECISION\n",
    "# Alpha diubah dari 0.55 menjadi 0.51 (mendekati 0.50)\n",
    "# Ini mengurangi bias terhadap kelas Judol, memaksa model lebih konservatif.\n",
    "NEW_ALPHA = 0.51 \n",
    "# Threshold diubah dari 0.65 menjadi 0.55.\n",
    "OPTIMAL_THRESHOLD = 0.55 \n",
    "# >>>\n",
    "TEXT_COLUMN = 'cleaned_comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "TEST_SIZE_FINAL = 0.20\n",
    "NEW_MODEL_AUG_PATH = \"judol_detection_high_precision.keras\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# INISIALISASI SASTRAWI\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword_remover = factory.create_stop_word_remover()\n",
    "factory_stemmer = StemmerFactory()\n",
    "stemmer = factory_stemmer.create_stemmer()\n",
    "\n",
    "# ===============================================\n",
    "# 1. DATA AUGMENTASI: SINTESIS JUDOL (LABEL 1)\n",
    "# ===============================================\n",
    "prefixes = ['slot', 'jp', 'wd', 'depo', 'gacor', 'raja', 'bosku', 'mantap']\n",
    "numbers = [str(i) for i in range(100, 999, 100)] + [str(i) for i in range(11, 99, 11)]\n",
    "phrases = ['gacor banget di sini!', 'pasti jp di situs ini', 'wd cepat tanpa ribet', 'cair terus bosku']\n",
    "\n",
    "synthetic_data_pos = []\n",
    "for i in range(200):\n",
    "    prefix = np.random.choice(prefixes)\n",
    "    num = np.random.choice(numbers)\n",
    "    phrase = np.random.choice(phrases)\n",
    "    brand_name = prefix + num\n",
    "    comment = f\"{phrase} coba main di {brand_name} aja deh\"\n",
    "    synthetic_data_pos.append({\"comment_text\": comment, \"target\": 1})\n",
    "\n",
    "df_synthetic_pos = pd.DataFrame(synthetic_data_pos)\n",
    "\n",
    "# ===============================================\n",
    "# 2. DATA AUGMENTASI: SINTESIS NON-JUDOL (LABEL 0) - PENTING UNTUK PRESISI\n",
    "# ===============================================\n",
    "NEGATIVE_AUGMENTATION_DATA = [\n",
    "    \"WD uang beasiswa sudah cair 5 juta alhamdulillah\",\n",
    "    \"WD dana dari platform saham tutorialnya jelas\",\n",
    "    \"WD gaji dari kantor total 10 juta\",\n",
    "    \"WD uang tunai 500 ribu dari ATM\",\n",
    "    \"Withdrawal dana P2P lending cair ke rekening\",\n",
    "    \"tarik uang 1 juta untuk beli buku kuliah\",\n",
    "    \"gacor banget motor tarikan mantap\",\n",
    "    \"gacor banget pelayanan toko ini\",\n",
    "    \"jp jelas puas sama produknya\",\n",
    "    \"motor baru ini gacor sekali\",\n",
    "    \"mobil jerman ini tarikannya mantap\",\n",
    "    \"jp (jelas puas) sama hasilnya\",\n",
    "    \"bosku beli kopi di warung\",\n",
    "    \"saya sudah deposit uang di bank\",\n",
    "    \"depo uang kuliah semester 3\",\n",
    "    \"akun ini sudah top up diamond\",\n",
    "    \"kami adalah agen perjalanan resmi\",\n",
    "]\n",
    "\n",
    "df_neg_aug = pd.DataFrame({'comment_text': NEGATIVE_AUGMENTATION_DATA, 'target': 0})\n",
    "\n",
    "print(f\"‚úÖ {len(df_synthetic_pos)} data Judol sintetik ditambahkan.\")\n",
    "print(f\"‚úÖ {len(df_neg_aug)} data Non-Judol (negatif) augmentasi ditambahkan.\")\n",
    "\n",
    "# ===============================================\n",
    "# 3. FUNGSI PREPROCESSING KONSISTEN\n",
    "# (Mencakup Normalisasi Samaran, Stopword, dan Stemming)\n",
    "# ===============================================\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning konsisten dengan Stemming dan Normalisasi Samaran.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Normalisasi Unicode & Konversi ke ASCII\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 2. Lowercase, Hapus URL/Mention\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+|@\\S+\", \"\", text)\n",
    "    \n",
    "    # 3. Hapus karakter yang bukan a-z, 0-9, atau spasi\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    \n",
    "    # --- NORMALISASI SAMARAN AGRESIF ---\n",
    "    text_parts = text.split()\n",
    "    processed_parts = []\n",
    "    i = 0\n",
    "    while i < len(text_parts):\n",
    "        current_part = text_parts[i]\n",
    "        j = i + 1\n",
    "        merged_word = current_part\n",
    "        while j < len(text_parts) and len(text_parts[j]) <= 1 and re.match(r'[a-z0-9]', text_parts[j]):\n",
    "            merged_word += text_parts[j]\n",
    "            j += 1\n",
    "        \n",
    "        if len(merged_word) > len(current_part):\n",
    "            processed_parts.append(merged_word)\n",
    "            i = j\n",
    "        else:\n",
    "            processed_parts.append(current_part)\n",
    "            i += 1\n",
    "\n",
    "    text = \" \".join(processed_parts)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # 4. Rapikan spasi dan trim\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 5. STOPWORD REMOVAL (Sastrawi)\n",
    "    text = stopword_remover.remove(text)\n",
    "    \n",
    "    # 6. STEMMING (Sastrawi)\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    # 7. Rapikan spasi lagi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 4. LOAD & GABUNGKAN DATA\n",
    "# ===============================================\n",
    "print(\"\\n--- 3. Memuat Data, Menggabungkan Augmentasi, dan Preprocessing ---\")\n",
    "try:\n",
    "    df_original = pd.read_csv(FILE_PATH)\n",
    "    df_original = df_original.dropna(subset=['comment_text', TARGET_COLUMN])\n",
    "    df_original[TARGET_COLUMN] = df_original[TARGET_COLUMN].astype(int)\n",
    "    \n",
    "    df_combined = pd.concat([\n",
    "        df_original[['comment_text', TARGET_COLUMN]], \n",
    "        df_synthetic_pos,\n",
    "        df_neg_aug\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    print(\"Menerapkan cleaning, stopword removal, dan stemming...\")\n",
    "    df_combined[TEXT_COLUMN] = df_combined['comment_text'].fillna('').progress_apply(clean_text_prep)\n",
    "    \n",
    "    df_combined = df_combined[df_combined[TEXT_COLUMN].str.len() > 0]\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: File '{FILE_PATH}' tidak ditemukan. Pastikan file berada di direktori yang sama.\")\n",
    "    sys.exit()\n",
    "\n",
    "X = df_combined[TEXT_COLUMN]\n",
    "Y = df_combined[TARGET_COLUMN]\n",
    "\n",
    "print(f\"\\nTotal data akhir setelah semua augmentasi dan cleaning: {len(X)} baris.\")\n",
    "print(f\"Jumlah Judol (1) sebelum split: {Y.sum()}\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 5. SPLIT DATA (STRATIFIED)\n",
    "# ===============================================\n",
    "print(\"\\n--- 4. Split Data (80% Train, 20% Test Stratified) ---\")\n",
    "X_train_text, X_test_text, Y_train, Y_test = train_test_split(\n",
    "    X, \n",
    "    Y, \n",
    "    test_size=TEST_SIZE_FINAL, \n",
    "    random_state=SEED, \n",
    "    stratify=Y \n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# 6. FEATURE EXTRACTION (Tokenisasi & Padding)\n",
    "# ===============================================\n",
    "print(\"\\n--- 5. Feature Extraction: Tokenisasi dan Padding ---\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text.astype(str))\n",
    "\n",
    "TOKENIZER_PATH = 'tokenizer_high_precision.pickle'\n",
    "with open(TOKENIZER_PATH, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"‚úÖ Tokenizer disimpan sebagai '{TOKENIZER_PATH}'\")\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text.astype(str))\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text.astype(str))\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 7. TERAPKAN SMOTE (HANYA PADA DATA LATIH)\n",
    "# ===============================================\n",
    "print(\"\\n--- 6. Terapkan SMOTE (Oversampling pada Data Latih) ---\")\n",
    "Y_train_np = Y_train.values \n",
    "smote = SMOTE(sampling_strategy='minority', random_state=SEED)\n",
    "\n",
    "X_resampled, Y_resampled = smote.fit_resample(X_train_padded, Y_train_np)\n",
    "\n",
    "X_train_final = np.array(X_resampled)\n",
    "Y_train_final = np.array(Y_resampled)\n",
    "\n",
    "judol_count = Y_train_final.sum()\n",
    "non_judol_count = len(Y_train_final) - judol_count\n",
    "print(f\"‚úÖ Data Latih Akhir Setelah SMOTE: {len(X_train_final)} baris (Rasio 1:1)\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 8. TRAINING DENGAN DATA SMOTE DAN FOCAL LOSS\n",
    "# ===============================================\n",
    "# Definisi Focal Loss harus dipertahankan karena digunakan di compile()\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # PERHATIAN: Loss dihitung dengan NEW_ALPHA = 0.51\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "    \n",
    "model_aug = Sequential([\n",
    "    Embedding(MAX_WORDS, EMBED_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# NOTE: Saat meng-compile, kita masih menggunakan custom loss\n",
    "model_aug.compile(loss=focal_loss(), optimizer=Adam(learning_rate=LR), metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "]\n",
    "\n",
    "print(f\"\\n--- 7. Memulai Pelatihan Model (Alpha={NEW_ALPHA} untuk Precision Tinggi) ---\")\n",
    "\n",
    "history = model_aug.fit(\n",
    "    X_train_final, Y_train_final, \n",
    "    validation_split=0.1, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "model_aug.save(\n",
    "    NEW_MODEL_AUG_PATH, \n",
    "    overwrite=True, \n",
    ")\n",
    "print(f\"\\nModel baru disimpan sebagai '{NEW_MODEL_AUG_PATH}' ‚úÖ\")\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# 9. EVALUASI DAN UJI MODEL BARU\n",
    "# ===============================================\n",
    "\n",
    "print(\"\\n=============================================\")\n",
    "print(\"üìä EVALUASI AKHIR MODEL PADA DATA UJI MURNI\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "Y_test_np = Y_test.values \n",
    "\n",
    "loss, accuracy = model_aug.evaluate(X_test_padded, Y_test_np, verbose=0) \n",
    "\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")\n",
    "print(f\"Akurasi pada Data Uji: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_proba = model_aug.predict(X_test_padded, verbose=0)\n",
    "y_pred_classes = (y_pred_proba > OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\nLaporan Klasifikasi (Threshold={OPTIMAL_THRESHOLD}):\")\n",
    "print(classification_report(Y_test_np, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072ab6c-c8b2-4b3a-be16-e18087a88a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# ===============================================\n",
    "# KONFIGURASI DAN INFERENSI\n",
    "# ===============================================\n",
    "MAX_LEN = 100\n",
    "# Menggunakan Alpha dan Threshold sebelumnya (0.51 dan 0.30)\n",
    "# Perubahan fokus pada perbaikan fitur (preprocessing)\n",
    "NEW_ALPHA = 0.51 \n",
    "OPTIMAL_THRESHOLD = 0.30 \n",
    "TOKENIZER_PATH = 'tokenizer_high_recall_weighted.pickle'\n",
    "MODEL_PATH = 'judol_detection_high_recall_weighted.keras'\n",
    "\n",
    "# TOKEN GENERIK BARU\n",
    "LINK_TOKEN = '[LINK]'\n",
    "BRAND_TOKEN = '[BRAND]'\n",
    "\n",
    "# INISIALISASI SASTRAWI\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword_remover = factory.create_stop_word_remover()\n",
    "factory_stemmer = StemmerFactory()\n",
    "stemmer = factory_stemmer.create_stemmer()\n",
    "\n",
    "# ===============================================\n",
    "# FUNGSI WAJIB UNTUK LOAD MODEL\n",
    "# (Harus identik dengan fungsi saat menyimpan model)\n",
    "# ===============================================\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    \"\"\"Definisi Focal Loss yang digunakan untuk load_model.\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        loss = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) \\\n",
    "                     - (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# ===============================================\n",
    "# FUNGSI PREPROCESSING KONSISTEN & GENERALISASI BRAND\n",
    "# ===============================================\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning dengan Stemming, Normalisasi Samaran, dan Masking Brand/Link.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Normalisasi Unicode & Konversi ke ASCII\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # 2. Lowercase, Hapus Mention\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    \n",
    "    # --- MASKING ENTITAS (HARUS SEBELUM HAPUS TANDA BACA/SPASI) ---\n",
    "    \n",
    "    # 2.1 MASKING URL/LINK: Ganti semua pola link dengan token generik [LINK]\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", LINK_TOKEN, text)\n",
    "    \n",
    "    # 3. Hapus karakter yang bukan a-z, 0-9, atau spasi (biarkan titik untuk samaran)\n",
    "    text = re.sub(r\"[^a-z0-9\\s.]\", \" \", text) \n",
    "    \n",
    "    # 4. NORMALISASI SAMARAN AGRESIF (s.l.o.t. -> slot, j.p -> jp)\n",
    "    # Hilangkan spasi di sekitar titik, lalu hilangkan titik\n",
    "    text = re.sub(r'\\s*\\.\\s*', '.', text)\n",
    "    text = text.replace('.', '')\n",
    "    # Rapikan spasi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 5. NORMALISASI SPASI PENDEK (Menggabungkan h t t p s, j p, w d)\n",
    "    text_parts = text.split()\n",
    "    processed_parts = []\n",
    "    i = 0\n",
    "    while i < len(text_parts):\n",
    "        current_part = text_parts[i]\n",
    "        j = i + 1\n",
    "        merged_word = current_part\n",
    "        \n",
    "        # Merge huruf tunggal yang terpisah (misalnya j p -> jp)\n",
    "        while j < len(text_parts) and len(text_parts[j]) <= 1 and re.match(r'[a-z0-9]', text_parts[j]):\n",
    "            merged_word += text_parts[j]\n",
    "            j += 1\n",
    "        \n",
    "        if len(merged_word) > len(current_part):\n",
    "             processed_parts.append(merged_word)\n",
    "             i = j\n",
    "        else:\n",
    "             processed_parts.append(current_part)\n",
    "             i += 1\n",
    "\n",
    "    text = \" \".join(processed_parts)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # 6. MASKING BRAND/NAMA SITUS: Ganti kata yang mengandung 3 atau lebih digit/angka berulang\n",
    "    def mask_brand(match):\n",
    "        word = match.group(0)\n",
    "        # Jika kata mengandung token LINK, jangan diubah\n",
    "        if LINK_TOKEN in word:\n",
    "            return word\n",
    "        # Pola umum brand Judol (angka berulang, angka di tengah)\n",
    "        if re.search(r'\\d{3,}|[a-z]{2,}\\d{2,}[a-z]*', word) and word not in ['hari', 'situs']:\n",
    "             return BRAND_TOKEN\n",
    "        return word\n",
    "\n",
    "    text = \" \".join([mask_brand(re.match(r'.*', word)) for word in text.split()])\n",
    "    text = text.replace(f\" {LINK_TOKEN}\", LINK_TOKEN).replace(f\"{LINK_TOKEN} \", LINK_TOKEN) # Clean up spaces around [LINK]\n",
    "    \n",
    "    # 7. STOPWORD REMOVAL (Sastrawi)\n",
    "    text = stopword_remover.remove(text)\n",
    "    \n",
    "    # 8. STEMMING (Sastrawi)\n",
    "    text = stemmer.stem(text)\n",
    "\n",
    "    # 9. Rapikan spasi lagi\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ===============================================\n",
    "# FUNGSI UTAMA PENGUJIAN\n",
    "# ===============================================\n",
    "\n",
    "def predict_judol(text, model, tokenizer, idx):\n",
    "    \"\"\"Memproses teks dan mengklasifikasikannya.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"[{idx+1}] TESTING | Teks Asli: '{text}'\")\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    cleaned_text = clean_text_prep(text)\n",
    "    print(f\"-> Teks Bersih: '{cleaned_text}'\")\n",
    "    \n",
    "    if not cleaned_text:\n",
    "        print(\"-> HASIL: Teks kosong setelah pembersihan.\")\n",
    "        print(\"=\" * 60)\n",
    "        return\n",
    "\n",
    "    # 2. Tokenisasi & Padding\n",
    "    seq = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    \n",
    "    # 3. Prediksi\n",
    "    probability = model.predict(padded, verbose=0)[0][0]\n",
    "    \n",
    "    # 4. Klasifikasi\n",
    "    prediction = 1 if probability > OPTIMAL_THRESHOLD else 0\n",
    "    \n",
    "    # 5. Output\n",
    "    result_map = {1: \"üö® BERPOTENSI JUDOL (TARGET 1)\", 0: \"‚úÖ BUKAN JUDOL (TARGET 0)\"}\n",
    "    \n",
    "    print(f\"-> Probabilitas Judol: {probability:.4f}\")\n",
    "    print(f\"-> Threshold Prediksi: {OPTIMAL_THRESHOLD}\")\n",
    "    print(f\"-> HASIL: {result_map[prediction]}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# LOAD ASET\n",
    "# ===============================================\n",
    "try:\n",
    "    print(f\"‚è≥ Memuat Tokenizer dari '{TOKENIZER_PATH}'...\")\n",
    "    with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    print(\"‚úÖ Tokenizer berhasil dimuat.\")\n",
    "    \n",
    "    print(f\"‚è≥ Memuat Model Keras dari '{MODEL_PATH}'...\")\n",
    "    # Penting: Menyediakan custom_objects untuk fungsi Focal Loss\n",
    "    model = load_model(\n",
    "        MODEL_PATH, \n",
    "        custom_objects={'loss': focal_loss(alpha=NEW_ALPHA)}\n",
    "    )\n",
    "    print(\"‚úÖ Model berhasil dimuat. Siap Uji Array.\")\n",
    "    print(f\"Menggunakan Threshold Prediksi: {OPTIMAL_THRESHOLD}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå ERROR: Pastikan file 'tokenizer_high_precision.pickle' dan 'judol_detection_high_precision.keras' ada di direktori yang sama.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# ARRAY TEST CASE (Mencakup skenario Brand-Only dan Samaran)\n",
    "# ===============================================\n",
    "TEST_CASES = [\n",
    "    \"Mantap bos, JP 100jt di situs gacor X Y Z langsung WD! Daftar sekarang.\", # Judol Jelas\n",
    "    \"JP lagi bosku, wd langsung cair di situs gacor SENSITIF999.\", # Judol Jelas (Brand-heavy)\n",
    "    \"s.l.o.t. g.a.c.o.r. jp hari ini 888\", # Judol Samaran (Brand di akhir)\n",
    "    \"WD uang gaji dari kantor sudah masuk rekening, alhamdulillah.\", # Non-Judol (Kata kunci umum)\n",
    "    \"Saya mau withdraw uang dari tabungan, apakah sudah bisa?\", # Non-Judol (Kata kunci umum)\n",
    "    \"Main game online itu menyenangkan, tapi jangan sampai lupa waktu.\", # Non-Judol\n",
    "    \"link ALTERNATIF DEPOSIT 10.000 GAMPANG JP: h t t p s://link.gacor.com/promo\", # Judol dengan Link Samaran\n",
    "    \"Mabar yuk, main game di server Asia!\", # Non-Judol\n",
    "    \"G.A.C.O.R 777 HARI INI PASTI CUAN\", # Judol Samaran dan Brand\n",
    "    \"Hanya di VIPWIN888, cuan setiap hari!\", # Brand-Only (Mengecek Generalisasi)\n",
    "    \"Situs paling jitu cuma di raja777, gak pake lama!\", # Brand-Only\n",
    "     \"Judi slot gacor hari ini pasti menang deposit 10k di link bio kami\", # JUDOL\n",
    "        \"Selamat siang semua, jangan lupa makan siang ya! Ini adalah cuaca yang cerah sekali.\", # NON-JUDOL\n",
    "        \"main di situs MPO444 sekarang dan langsung withdraw! dijamin jp\", # JUDOL\n",
    "        \"video ini sangat bagus dan mengedukasi sekali tentang sejarah peradaban kuno di mesir.\", # NON-JUDOL\n",
    "        \"Ayo join situs bola aman 11betting terpercaya di indonesia dan dapatkan bonus terbesar hari ini.\", # JUDOL\n",
    "        \"Kapan episode selanjutnya tayang? Menunggu banget nih. Kami ingin tahu kelanjutannya.\", # NON-JUDOL\n",
    "        \"link alternatif 77slot gampang menang nih bosku. klik link di bio.\", # JUDOL\n",
    "        \"Selamat ulang tahun untuk teman baikku! Semoga panjang umur dan sehat selalu ya.\", # NON-JUDOL\n",
    "        \"Ayo buruan daftar di MPO 7 77 sekarang, dapatkan bonus terbesar!\", # JUDOL\n",
    "        \"WD 500rb di GACOR777 mantap langsung cair!\", # JUDOL\n",
    "        \"buruan daftar di MANTAPJP888, depo 100k langsung dapat bonus!\", # JUDOL\n",
    "        \"main slot pasti untung, depo terus bosku\", # JUDOL\n",
    "        \"JP boss 44D mantap\", # NON-JUDOL\n",
    "        \"S L 0 T P R 1 M A 8 8 8. Jp 100k wd cepat.\", # JUDOL\n",
    "        \"Cepat gabung w1n k3lub, b4rus4n w d s4ld0 5j.\", # JUDOL\n",
    "        \"Lagi gacor main di maxwin888 hari ini. Lomba makan.\", # JUDOL (Target perbaikan 1)\n",
    "        \"Top banget! Terimakasih ùóõùó¢ùóßùóùùóòùó£ùóòùü±ùü± GOOD JOB!üÄÑü•∞\", # JUDOL (Target perbaikan 2)\n",
    "        \"WD 1O jt di BOSSku777. *lgsg cair*!\", # JUDOL\n",
    "        \"Depo 100k, WD 20juta di RAJASLOT777. Pasti JP!\", # JUDOL\n",
    "        \"Aku di MANTAP888. Gacor terus!\", # JUDOL (Target perbaikan 3)\n",
    "        \"buruan daftar di MANTAPJEPE888 promonya gede banget\", # JUDOL\n",
    "        \"Paling the best! HOTJEPE55 Memang GOOD JOB banget!\", # JUDOL (Target perbaikan 4)\n",
    "        \"main di slotvip4d aja, depo 50 wd 500\", # JUDOL\n",
    "        \"gacorwd77 adalah situs paling oke.\", # JUDOL (Target perbaikan 5)\n",
    "        \"wd 1000 jp. link ada di bio ya.\", # JUDOL\n",
    "        \"jp 2OOm jt sudah di WD. Paling gacor di sini.\", # JUDOL\n",
    "        \"S L O T G A C O R 1 2 3 . C O M. Puji Tuhan, rejeki nomplok\", # JUDOL\n",
    "        \"asli keren ùóöùóîùóüùóîùóïùóòùóßùü≤ùü¥ùü¥ selalu bikin hepyy.\", # JUDOL (Target perbaikan 6)\n",
    "        \"tokowin303 4w1 likeReply\", # JUDOL (Target perbaikan 7)\n",
    "        \"Saya mau ngasih saran yg baik ya bg kemaren saya main di ùôàùòºùôâùôèùôêùôáùü∞ùü¨ùü∞ di kasih 22juta\", # JUDOL (Target perbaikan 8)\n",
    "        \"ùóüùóîùóïùóîùüµùü≠ emang the real MVP, selalu bikin kita bahagia dengan kemenangan emang the real MVP, selalu bikin kita bahagia dengan kemenangan\", # JUDOL (Target perbaikan 9)\n",
    "        \"slotvip777 depo 50 wd 500\", # JUDOL\n",
    "        \"JP boss 44D mantap\", # NON-JUDOL\n",
    "        \"üí∞ WD 5üü¢0üü¢0k di P U L A U W I N 88! Langsung ü§ë JP! Lomba\", # JUDOL\n",
    "        \"S L 0 T G A C 0 R 1 2 3 --- jp 500k wd cepat\", # JUDOL\n",
    "        \"ùó†ùóîùó°ùóßùóîùó£ùóùùóòùó£ùóòùü¥ùü¥üî•üî• WD 2ùü¨ùü¨j\", # JUDOL\n",
    "        \"W D¬† ¬†1 0 j t .. d i ùóïùó¢ùó¶ùó¶ùóûùòÇ777 !!\", # JUDOL\n",
    "        \"jp besar üíµ WD kecil. Main di s l 0 t v 1 p 1 1 1 sekarang!\", # JUDOL\n",
    "        \"p_œÖ_l_Œ±_œÖ_w_Œπ_n88 üí∞ jp 10J cepat WD!!\", # JUDOL\n",
    "        \"garudahokiüî• mantapjepe888 üèÜ jp 100k wd lancar\", # JUDOL\n",
    "        \"wd sepuluh juta di slotvip111 üéØ langsung cair\", # JUDOL\n",
    "        \"wd cepat + JP 200k main di RAJASLOT777 sekarang\", # JUDOL\n",
    "        \"Daft4r di S G 1 ‚Äß 8‚Äß8‚Äß bentar lagi ada drop mantap.\", # JUDOL\n",
    "        \"mAin di bùöòùöúùöîùöû7ùüΩùüΩ enak bet, wd aman.\", # JUDOL\n",
    "        \"bo.s ku. tujuh tujuh tujuh lagi gila2an promo wdee.\", # JUDOL\n",
    "        \"m‚Äãa‚Äãi‚Äãn‚Äã ‚Äãd‚Äãi‚Äã ‚Äãb‚Äão‚Äãs‚Äãk‚Äãu‚Äã7‚Äã7‚Äã7‚Äã g‚Äãa‚Äãs‚Äãk‚Äãe‚Äãn‚Äã.\", # JUDOL\n",
    "        \"Coba deh di \\\"QlRTT1QxMDA=\\\" katanya jam 3 pagi gacor.\", # JUDOL\n",
    "        \"Main di cptlv888 lumayan tarikannya. Lomba\", # JUDOL (Target perbaikan 10)\n",
    "        \"Êàë Âú® Âçö Âè∏ Ëã¶ ‰∏ÉÊòü ‰∏ÉÊòü ‰∏ÉÊòü Âàö Âèñ Ê¨æ.\", # NON-JUDOL\n",
    "        \"b0sku77? gas bet wd ngebut bgt.\", # JUDOL\n",
    "        \"b o s k u 7 7 7 ...disusun vertikal.\", # JUDOL\n",
    "        \"WD 600 tadi, no cap. That site hits diff fr.\", # JUDOL\n",
    "        \"M@!N di B0$KU^777 cuy wd kuzz teroosss.\", # JUDOL\n",
    "        \"bo sku 7 7 7 ‚Äî karena kamera burem jd gini.\", # JUDOL\n",
    "        \"Beli skin game baru ah, keren nih!\", # NON-JUDOL\n",
    "        \"Ini channel YouTube favoritku, sering update video baru.\", # NON-JUDOL\n",
    "        \"Makan malam apa hari ini?\", # NON-JUDOL\n",
    "        \"Saya mau nonton film gratis di y0utube, tapi juga WD 5 ribu dari tabungan\", # NON-JUDOL\n",
    "        \"WD 50 ribu saja, buat beli kopi.\", # NON-JUDOL\n",
    "        \"wah mantap nih, motor baru udah di WD sama pemiliknya\", # NON-JUDOL\n",
    "        \"itu tempat bosku yang warna hijau itu loh, yg sering iklan jam 2 pagi.\", # JUDOL (Target perbaikan 11)\n",
    "        \"Tadi malem aku pecah telur di tempat sebelah. Tarikan bersih bgt.\", # JUDOL\n",
    "        \"Udah, masuk aja ke yang kemarin aku bilang. Yang tiap hari bagi hadiah itu.\", # JUDOL\n",
    "        \"Wkwk mantap jp lagi di ‚Äúplatform terpercaya‚Äù itu katanya ü§°\", # NON-JUDOL\n",
    "        \"bos ku sudah datang tadi pagi bawa hadiah 25k. Gila.\", # NON-JUDOL\n",
    "        \"Ingat, keberanian itu modal. Kalau mau hasil besar, tempat itu udah aku rekomendasiin kemarin.\", # JUDOL\n",
    "        \"Semoga sukses selalu dan sehat terus ya. Kontennya sangat bermanfaat!\", # NON-JUDOL\n",
    "        \"Saya transfer 500 ribu ke rekening teman tadi pagi, semoga lancar.\", # NON-JUDOL\n",
    "        \"Main Mobile Legends seru banget, tapi harus top up dulu buat beli diamond.\", # NON-JUDOL\n",
    "        \"Saya pesan nasi Padang di warung Bossku tadi, rasanya mantap.\", # NON-JUDOL\n",
    "        \"Saya sudah JP (Jelas Puas) sama pelayanan toko ini.\", # NON-JUDOL\n",
    "        \"Komentar ini hanya berisi pujian, tidak ada unsur promosi atau judi.\", # NON-JUDOL\n",
    "        \"link untuk download materi kuliah ada di deskripsi video ya teman-teman.\", # NON-JUDOL\n",
    "        \"Terima kasih atas video tutorial cara WD (Withdrawal) uang dari platform saham.\", # NON-JUDOL\n",
    "        \"Dijamin happy udah kaya rumah kedua ùêÄùêëùêÑùêçùêÄùüóùüêùüë disini üëè\", # JUDOL (Target perbaikan 12)\n",
    "        \"cuma di tempat lu bang yang paling amanahüôå\", # JUDOL\n",
    "        \"ùôàùôÄùôÇùòºùòΩùôÄùôèùü≠ùü¥ùüµ emang the real MVP selalu bikin kita bahagia dengan kemenangannya\", # JUDOL (Target perbaikan 13)\n",
    "        \"Tadi pagi aku WD (Withdraw) uang tunai dari ATM 500 ribu.\", # NON-JUDOL\n",
    "        \"WD dana beasiswa sudah cair, total 5 juta! Alhamdulillah.\", # NON-JUDOL\n",
    "        \"Jangan lupa WD hasil penjualan dari platform e-commerce itu ya.\", # NON-JUDOL\n",
    "        \"Pelayanan toko ini JP (Jelas Puas) banget, barangnya 'Gacor'!\", # NON-JUDOL\n",
    "        \"Aku JP (Jaminan Produk) 100% kalau kamu beli di Bossku Official Store.\", # NON-JUDOL\n",
    "        \"Wah, si Bossku tadi pagi datang bawa hadiah, mantap JP!\", # NON-JUDOL\n",
    "        \"Motor baruku Gacor banget, tarikannya mantap!\", # NON-JUDOL\n",
    "        \"Si Bossku (panggilan teman) tadi Depo (Deposit) makanan ke meja.\", # NON-JUDOL\n",
    "        \"Restoran ini Gacor tiap hari, ramenya nggak ketulungan.\", # NON-JUDOL\n",
    "        \"Aku dapat promo Depo 50k, dapat 50k di Toko X (bukan situs judi).\", # NON-JUDOL\n",
    "        \"Gacor banget, main game Mobile Legends dapat JP (Juara Pertama).\", # NON-JUDOL\n",
    "        \"Kata Bossku, ini adalah situs terbaik untuk beli akun game (bukan judi).\", # NON-JUDOL\n",
    "        \"WD 5 ribu aja buat beli kopi, soalnya dompet udah 'Gacor'.\" # NON-JUDOL\n",
    "]\n",
    "\n",
    "def array_test_loop():\n",
    "    print(\"--- MEMULAI PENGUJIAN DENGAN ARRAY INPUT ---\")\n",
    "    for i, text in enumerate(TEST_CASES):\n",
    "        predict_judol(text, model, tokenizer, i)\n",
    "    print(\"--- PENGUJIAN ARRAY SELESAI ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    array_test_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68b0d6-8a6b-4879-813e-aa12d79aca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import re\n",
    "import unicodedata\n",
    "import pickle\n",
    "import sys\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tqdm import tqdm # Import tqdm untuk progress bar\n",
    "\n",
    "# Aktifkan progress bar untuk operasi Pandas/Series\n",
    "tqdm.pandas() \n",
    "\n",
    "# ===============================================\n",
    "# KONFIGURASI DAN HYPERPARAMETER\n",
    "# ===============================================\n",
    "DATA_FILE = 'production_judol_detection_old.csv'\n",
    "MODEL_PATH = 'judol_detection_high_recall_weighted.keras' \n",
    "TOKENIZER_PATH = 'tokenizer_high_recall_weighted.pickle'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "MAX_LEN = 100\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 64\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Alpha baru untuk Focal Loss: \n",
    "NEW_ALPHA = 0.35 \n",
    "\n",
    "# TOKEN GENERIK UNTUK MASKING\n",
    "LINK_TOKEN = '[LINK]'\n",
    "BRAND_TOKEN = '[BRAND]'\n",
    "\n",
    "# INISIALISASI SASTRAWI\n",
    "try:\n",
    "    factory = StopWordRemoverFactory()\n",
    "    stopword_remover = factory.create_stop_word_remover()\n",
    "    factory_stemmer = StemmerFactory()\n",
    "    stemmer = factory_stemmer.create_stemmer()\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Peringatan: Library Sastrawi mungkin tidak terinstal. Preprocessing akan dibatasi.\")\n",
    "    stopword_remover = lambda x: x\n",
    "    stemmer = lambda x: x\n",
    "\n",
    "# ===============================================\n",
    "# FUNGSI PREPROCESSING KONSISTEN\n",
    "# ===============================================\n",
    "\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning dengan Stemming, Normalisasi Samaran, dan Masking Brand/Link.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    \n",
    "    # MASKING URL/LINK\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", LINK_TOKEN, text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-z0-9\\s.]\", \" \", text) \n",
    "    text = re.sub(r'\\s*\\.\\s*', '.', text)\n",
    "    text = text.replace('.', '')\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # NORMALISASI SPASI PENDEK\n",
    "    text_parts = text.split()\n",
    "    processed_parts = []\n",
    "    i = 0\n",
    "    while i < len(text_parts):\n",
    "        current_part = text_parts[i]\n",
    "        j = i + 1\n",
    "        merged_word = current_part\n",
    "        \n",
    "        while j < len(text_parts) and len(text_parts[j]) <= 1 and re.match(r'[a-z0-9]', text_parts[j]):\n",
    "            merged_word += text_parts[j]\n",
    "            j += 1\n",
    "        \n",
    "        if len(merged_word) > len(current_part):\n",
    "             processed_parts.append(merged_word)\n",
    "             i = j\n",
    "        else:\n",
    "             processed_parts.append(current_part)\n",
    "             i += 1\n",
    "\n",
    "    text = \" \".join(processed_parts)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # MASKING BRAND/NAMA SITUS\n",
    "    def mask_brand(word):\n",
    "        if LINK_TOKEN in word:\n",
    "            return word\n",
    "        if re.search(r'\\d{3,}|[a-z]{2,}\\d{2,}[a-z]*', word) and word not in ['hari', 'situs', 'tahun', 'minggu']:\n",
    "             return BRAND_TOKEN\n",
    "        return word\n",
    "\n",
    "    text = \" \".join([mask_brand(word) for word in text.split()])\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # STOPWORD REMOVAL & STEMMING\n",
    "    if stopword_remover is not None:\n",
    "        text = stopword_remover.remove(text)\n",
    "    \n",
    "    if stemmer is not None:\n",
    "        text = stemmer.stem(text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# ===============================================\n",
    "# FUNGSI LOSS BARU (FOCAL LOSS)\n",
    "# ===============================================\n",
    "\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    \"\"\"Focal Loss dengan alpha rendah untuk Recall Tinggi.\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        loss_pos = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n",
    "        loss_neg = -(1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        return tf.reduce_mean(loss_pos + loss_neg)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# ===============================================\n",
    "# PELATIHAN MODEL UTAMA DENGAN CLASS WEIGHTS\n",
    "# ===============================================\n",
    "\n",
    "def train_judol_model():\n",
    "    print(\"--- MEMULAI PELATIHAN MODEL REKAL TINGGI DENGAN BOBOT KELAS ---\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: File data '{DATA_FILE}' tidak ditemukan.\")\n",
    "        return\n",
    "\n",
    "    REQUIRED_COLUMNS = ['combined_text', 'target']\n",
    "    if not all(col in df.columns for col in REQUIRED_COLUMNS):\n",
    "        print(f\"‚ùå ERROR: Data tidak memiliki kolom yang wajib: {REQUIRED_COLUMNS}\")\n",
    "        return\n",
    "\n",
    "    # 1. Preprocessing Data\n",
    "    print(\"‚è≥ Menerapkan Preprocessing ke Data...\")\n",
    "    # Menggunakan .progress_apply() dari tqdm\n",
    "    df['text_cleaned'] = df['combined_text'].astype(str).progress_apply(clean_text_prep)\n",
    "    \n",
    "    df_clean = df[df['text_cleaned'].str.len() > 0].copy()\n",
    "    df_clean['target'] = df_clean['target'].fillna(0).astype(int)\n",
    "\n",
    "    texts = df_clean['text_cleaned'].tolist()\n",
    "    labels = df_clean['target'].values\n",
    "    \n",
    "    print(f\"Total Sampel yang Digunakan: {len(df_clean)}\")\n",
    "    print(\"\\n--- Distribusi Label Data Lengkap ---\")\n",
    "    label_counts = df_clean['target'].value_counts()\n",
    "    print(label_counts)\n",
    "\n",
    "    # 2. Split Data (Stratified 80% Train, 20% Test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.20, random_state=RANDOM_STATE, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # 3. MENGHITUNG BOBOT KELAS (CLASS WEIGHTS)\n",
    "    print(\"\\n‚è≥ Menghitung Bobot Kelas untuk mengatasi ketidakseimbangan...\")\n",
    "    \n",
    "    weights = class_weight.compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = {i: weights[i] for i in range(len(weights))}\n",
    "    \n",
    "    print(f\"Bobot Kelas yang Dihitung (Class Weights):\")\n",
    "    print(f\"Kelas 0 (Non-Judol/Mayoritas): {class_weight_dict.get(0):.3f}\")\n",
    "    print(f\"Kelas 1 (Judol/Minoritas): {class_weight_dict.get(1):.3f}\")\n",
    "    \n",
    "    # 4. Tokenisasi\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    if LINK_TOKEN not in tokenizer.word_index:\n",
    "        tokenizer.word_index[LINK_TOKEN] = len(tokenizer.word_index) + 1\n",
    "    if BRAND_TOKEN not in tokenizer.word_index:\n",
    "        tokenizer.word_index[BRAND_TOKEN] = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    padded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "    # 5. Membangun Model LSTM\n",
    "    print(\"\\n‚è≥ Membangun Model LSTM...\")\n",
    "    model = Sequential([\n",
    "        Embedding(len(tokenizer.word_index) + 1, EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "        LSTM(LSTM_UNITS, return_sequences=True),\n",
    "        LSTM(LSTM_UNITS),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "\n",
    "    # 6. Kompilasi dengan Focal Loss\n",
    "    print(f\"‚è≥ Mengkompilasi model dengan Focal Loss (alpha={NEW_ALPHA})...\")\n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss=focal_loss(gamma=2.0, alpha=NEW_ALPHA), \n",
    "        metrics=['accuracy', tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.Precision(name='precision')]\n",
    "    )\n",
    "\n",
    "    # 7. Pelatihan dengan Class Weights\n",
    "    print(\"‚è≥ Melatih model dengan Class Weights yang diterapkan...\")\n",
    "    model.fit(\n",
    "        padded_train_sequences, \n",
    "        y_train, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        validation_split=0.1, \n",
    "        class_weight=class_weight_dict, \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 8. Menyimpan Model dan Tokenizer\n",
    "    model.save(MODEL_PATH)\n",
    "    with open(TOKENIZER_PATH, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"\\n‚úÖ PELATIHAN SELESAI!\")\n",
    "    print(f\"Model Rekal Tinggi tersimpan di: {MODEL_PATH}\")\n",
    "    print(f\"Tokenizer tersimpan di: {TOKENIZER_PATH}\")\n",
    "    \n",
    "    df_test = pd.DataFrame({'combined_text': X_test, 'target': y_test})\n",
    "    df_test.to_csv('testing_data_split_weighted.csv', index=False)\n",
    "    print(f\"Data testing (20% dari dataset) disimpan di 'testing_data_split_weighted.csv' untuk evaluasi.\")\n",
    "    print(\"\\n**LANGKAH PENGUJIAN:** Jalankan 'judol_testing_high_recall_weighted.py' dengan threshold rendah (0.20) untuk mendapatkan Rekal maksimum!\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_judol_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f02d2-25ff-48fc-a71d-acd35fbb0169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix, classification_report\n",
    "import pickle\n",
    "import unicodedata\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import Sastrawi secara opsional jika diperlukan untuk fungsi preprocessing\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "    factory_stemmer = StemmerFactory()\n",
    "    stemmer = factory_stemmer.create_stemmer()\n",
    "    factory_stopword = StopWordRemoverFactory()\n",
    "    stopword_remover = factory_stopword.create_stop_word_remover()\n",
    "except ImportError:\n",
    "    # Fallback jika Sastrawi tidak tersedia, Preprocessing akan terpengaruh\n",
    "    stemmer = None\n",
    "    stopword_remover = None\n",
    "    print(\"‚ö†Ô∏è Peringatan: Sastrawi tidak ditemukan. Stemming/Stopword Removal dinonaktifkan.\")\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- 1. KONFIGURASI DAN HYPERPARAMETERS KONSISTEN ---\n",
    "# HARUS SAMA PERSIS dengan skrip pelatihan Anda\n",
    "MAX_LEN = 100\n",
    "NEW_ALPHA = 0.35 \n",
    "\n",
    "# Ambang Batas Klasifikasi (Threshold) untuk Recall Tinggi\n",
    "# Nilai rendah ini adalah STRATEGI untuk High Recall.\n",
    "OPTIMAL_THRESHOLD = 0.20 \n",
    "\n",
    "# Jalur File\n",
    "MODEL_PATH = 'judol_detection_high_recall_weighted.keras'\n",
    "TOKENIZER_PATH = 'tokenizer_high_recall_weighted.pickle'\n",
    "DATA_FILE = 'testing_data_split_weighted.csv' # Data testing yang dihasilkan skrip pelatihan\n",
    "\n",
    "# TOKEN GENERIK KONSISTEN\n",
    "LINK_TOKEN = '[LINK]'\n",
    "BRAND_TOKEN = '[BRAND]'\n",
    "\n",
    "# --- 2. FUNGSI PREPROCESSING KONSISTEN (Duplikat dari Skrip Pelatihan) ---\n",
    "def clean_text_prep(text):\n",
    "    \"\"\"Fungsi cleaning dengan Stemming, Normalisasi Samaran, dan Masking Brand/Link.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"@\\S+\", \"\", text)\n",
    "    \n",
    "    # MASKING URL/LINK\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", LINK_TOKEN, text)\n",
    "    \n",
    "    text = re.sub(r\"[^a-z0-9\\s.]\", \" \", text) \n",
    "    text = re.sub(r'\\s*\\.\\s*', '.', text)\n",
    "    text = text.replace('.', '')\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # NORMALISASI SPASI PENDEK\n",
    "    text_parts = text.split()\n",
    "    processed_parts = []\n",
    "    i = 0\n",
    "    while i < len(text_parts):\n",
    "        current_part = text_parts[i]\n",
    "        j = i + 1\n",
    "        merged_word = current_part\n",
    "        \n",
    "        while j < len(text_parts) and len(text_parts[j]) <= 1 and re.match(r'[a-z0-9]', text_parts[j]):\n",
    "            merged_word += text_parts[j]\n",
    "            j += 1\n",
    "        \n",
    "        if len(merged_word) > len(current_part):\n",
    "              processed_parts.append(merged_word)\n",
    "              i = j\n",
    "        else:\n",
    "              processed_parts.append(current_part)\n",
    "              i += 1\n",
    "\n",
    "    text = \" \".join(processed_parts)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    # MASKING BRAND/NAMA SITUS\n",
    "    def mask_brand(word):\n",
    "        if LINK_TOKEN in word:\n",
    "            return word\n",
    "        # Kondisi: kombinasi 3+ digit ATAU (2+ huruf diikuti 2+ digit diikuti 0/lebih huruf)\n",
    "        # dan BUKAN kata-kata pengecualian\n",
    "        if re.search(r'\\d{3,}|[a-z]{2,}\\d{2,}[a-z]*', word) and word not in ['hari', 'situs', 'tahun', 'minggu']:\n",
    "             return BRAND_TOKEN\n",
    "        return word\n",
    "\n",
    "    text = \" \".join([mask_brand(word) for word in text.split()])\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # STOPWORD REMOVAL & STEMMING (gunakan instance Sastrawi yang dimuat di awal)\n",
    "    if stopword_remover is not None:\n",
    "        text = stopword_remover.remove(text)\n",
    "    \n",
    "    if stemmer is not None:\n",
    "        text = stemmer.stem(text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# --- 3. FUNGSI LOSS KONSISTEN (Wajib untuk Pemuatan Model) ---\n",
    "def focal_loss(gamma=2., alpha=NEW_ALPHA):\n",
    "    \"\"\"Focal Loss dengan alpha rendah untuk Recall Tinggi.\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        loss_pos = -alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n",
    "        loss_neg = -(1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        return tf.reduce_mean(loss_pos + loss_neg)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# --- 4. EKSEKUSI UTAMA PENGUJIAN ---\n",
    "def test_judol_model():\n",
    "    print(\"--- MEMULAI PENGUJIAN MODEL REKAL TINGGI DENGAN THRESHOLD RENDAH ---\")\n",
    "    \n",
    "    # Pemuatan Tokenizer\n",
    "    try:\n",
    "        with open(TOKENIZER_PATH, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "        print(f\"‚úÖ Tokenizer berhasil dimuat dari: {TOKENIZER_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Tokenizer '{TOKENIZER_PATH}' tidak ditemukan. Jalankan pelatihan terlebih dahulu.\")\n",
    "        return\n",
    "    \n",
    "    # Pemuatan Model dan Dependencies\n",
    "    try:\n",
    "        custom_objects = {'loss': focal_loss(gamma=2., alpha=NEW_ALPHA)}\n",
    "        model = load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "        print(f\"‚úÖ Model berhasil dimuat dari: {MODEL_PATH}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Model '{MODEL_PATH}' tidak ditemukan. Jalankan pelatihan terlebih dahulu.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR saat memuat model: Pastikan fungsi 'focal_loss' sama persis.\")\n",
    "        print(f\"Detail Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Pemuatan Data Uji (Split)\n",
    "    try:\n",
    "        df_test = pd.read_csv(DATA_FILE)\n",
    "        X_test_raw = df_test['combined_text'].fillna('')\n",
    "        y_test = df_test['target'].values\n",
    "        print(f\"‚úÖ Data uji ({len(df_test)} sampel) berhasil dimuat dari: {DATA_FILE}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Data uji '{DATA_FILE}' tidak ditemukan. Ini dibuat setelah pelatihan.\")\n",
    "        return\n",
    "\n",
    "    # --- PREPROCESSING DATA UJI ---\n",
    "    print(\"\\n[STEP 1/3] Menerapkan Preprocessing ke Data Uji...\")\n",
    "    X_test_processed = X_test_raw.progress_apply(clean_text_prep)\n",
    "    \n",
    "    # --- TOKENISASI DAN PADDING ---\n",
    "    print(\"[STEP 2/3] Menerapkan Tokenisasi dan Padding...\")\n",
    "    \n",
    "    # Konversi teks menjadi sequence menggunakan tokenizer yang sudah dilatih\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test_processed)\n",
    "    \n",
    "    # Padding\n",
    "    X_test_padded = pad_sequences(\n",
    "        X_test_sequences, \n",
    "        maxlen=MAX_LEN, \n",
    "        padding='post', \n",
    "        truncating='post'\n",
    "    )\n",
    "\n",
    "    # --- PREDIKSI MODEL ---\n",
    "    print(\"\\n[STEP 3/3] Melakukan Prediksi dengan Model...\")\n",
    "    y_pred_proba = model.predict(X_test_padded)\n",
    "    \n",
    "    # Ekstrak probabilitas untuk kelas 1 (Judol)\n",
    "    y_pred_proba = y_pred_proba.flatten() \n",
    "\n",
    "    # --- EVALUASI DENGAN THRESHOLD TINGGI RECALL ---\n",
    "    print(f\"\\n--- EVALUASI KINERJA MODEL (Threshold: {OPTIMAL_THRESHOLD}) ---\")\n",
    "    print(\"Tujuan: Rekal Tinggi (Minimalisir False Negatives)\")\n",
    "    \n",
    "    # Klasifikasi menggunakan threshold rendah\n",
    "    y_pred_classified = (y_pred_proba >= OPTIMAL_THRESHOLD).astype(int)\n",
    "\n",
    "    # Hitung Metriks\n",
    "    recall = recall_score(y_test, y_pred_classified)\n",
    "    precision = precision_score(y_test, y_pred_classified)\n",
    "    f1 = f1_score(y_test, y_pred_classified)\n",
    "    cm = confusion_matrix(y_test, y_pred_classified)\n",
    "\n",
    "    # Tampilkan Hasil\n",
    "    print(\"\\n[Metriks Utama Model Judol]\")\n",
    "    print(f\"üéØ Recall (Rekal) @ {OPTIMAL_THRESHOLD}: {recall:.4f}\")\n",
    "    print(f\"Presisi (Precision) @ {OPTIMAL_THRESHOLD}: {precision:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n[Matriks Kebingungan (Confusion Matrix)]\")\n",
    "    print(\"  | Prediksi Non-Judol | Prediksi Judol\")\n",
    "    print(\"---|--------------------|----------------\")\n",
    "    print(f\"Nyata Non-Judol | {cm[0, 0]:<18} | {cm[0, 1]:<12} (False Positives)\")\n",
    "    print(f\"Nyata Judol     | {cm[1, 0]:<18} (False Negatives) | {cm[1, 1]:<12}\")\n",
    "    \n",
    "    print(\"\\n[Laporan Klasifikasi Terperinci]\")\n",
    "    print(classification_report(y_test, y_pred_classified, target_names=['Non-Judol', 'Judol']))\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_judol_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b9342-45e4-4a66-bf54-aca9e315a6d6",
   "metadata": {},
   "source": [
    "# KODE GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d95720d4-3c20-401d-ae8b-525661ea21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 19252\n",
      "Augmented: 35406\n",
      "Total: 54658\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# =====================================\n",
    "# 1. LOAD DATA\n",
    "# =====================================\n",
    "df = pd.read_csv(\"production_judol_detection_old.csv\")\n",
    "\n",
    "# pastikan kolom benar\n",
    "TEXT_COL = \"cleaned_comment_text\"\n",
    "LABEL_COL = \"target\"\n",
    "\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna()\n",
    "\n",
    "# =====================================\n",
    "# 2. BRAND LIST\n",
    "# =====================================\n",
    "brand_list = [\n",
    "    \"cukong\", \"maxwin\", \"slot88\", \"situs gacor\", \"gacor\", \"winrate\",\n",
    "    \"slot online\", \"slot dana\", \"slot deposit\", \"slot pakai pulsa\",\n",
    "    \"gates olympus\", \"olympus\", \"pragmatic\", \"habanero\",\n",
    "]\n",
    "\n",
    "# bikin regex brand ‚Üí <BRAND>\n",
    "brand_regex = re.compile(\n",
    "    r\"\\b(\" + \"|\".join([re.escape(b) for b in brand_list]) + r\")\\b\", flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def replace_brands(text):\n",
    "    return brand_regex.sub(\" <BRAND> \", text)\n",
    "\n",
    "# =====================================\n",
    "# 3. DATA AUGMENTATION\n",
    "# =====================================\n",
    "def augment_text(text):\n",
    "    aug = []\n",
    "\n",
    "    # 1. Swap word random\n",
    "    words = text.split()\n",
    "    if len(words) > 3:\n",
    "        i, j = random.sample(range(len(words)), 2)\n",
    "        swapped = words.copy()\n",
    "        swapped[i], swapped[j] = swapped[j], swapped[i]\n",
    "        aug.append(\" \".join(swapped))\n",
    "\n",
    "    # 2. Insert <BRAND> random\n",
    "    if random.random() < 0.3:\n",
    "        idx = random.randint(0, len(words))\n",
    "        insert_brand = words[:idx] + [\"<BRAND>\"] + words[idx:]\n",
    "        aug.append(\" \".join(insert_brand))\n",
    "\n",
    "    # 3. Duplicate random word\n",
    "    if len(words) > 2:\n",
    "        k = random.randint(0, len(words)-1)\n",
    "        dup = words.copy()\n",
    "        dup.insert(k, words[k])\n",
    "        aug.append(\" \".join(dup))\n",
    "\n",
    "    return aug\n",
    "\n",
    "\n",
    "# apply brand replacement\n",
    "df[\"text_norm\"] = df[TEXT_COL].astype(str).apply(replace_brands)\n",
    "\n",
    "# generate augment\n",
    "aug_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    text = row[\"text_norm\"]\n",
    "    label = row[LABEL_COL]\n",
    "\n",
    "    for augmented in augment_text(text):\n",
    "        aug_rows.append([augmented, label])\n",
    "\n",
    "df_aug = pd.DataFrame(aug_rows, columns=[\"text_norm\", LABEL_COL])\n",
    "\n",
    "# combine original + augmented\n",
    "df_final = pd.concat([df[[\"text_norm\", LABEL_COL]], df_aug], ignore_index=True)\n",
    "\n",
    "print(\"Original:\", len(df))\n",
    "print(\"Augmented:\", len(df_aug))\n",
    "print(\"Total:\", len(df_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e597db87-edc2-42e0-9647-417c7587b77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 43726\n",
      "Val: 10932\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# TOKENIZER\n",
    "# =====================================\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(df_final[\"text_norm\"])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df_final[\"text_norm\"])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN)\n",
    "\n",
    "y = df_final[LABEL_COL].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", len(X_train))\n",
    "print(\"Val:\", len(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a0bbdd35-51e5-4a22-a547-41a06dfd6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wtf/anaconda3/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                    </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">       Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_20                ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            ‚îÇ ?                      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_21                ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            ‚îÇ ?                      ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                ‚îÇ ?                      ‚îÇ   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding_24 (\u001b[38;5;33mEmbedding\u001b[0m)        ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_20                ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBidirectional\u001b[0m)                 ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            ‚îÇ ?                      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ bidirectional_21                ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îÇ (\u001b[38;5;33mBidirectional\u001b[0m)                 ‚îÇ                        ‚îÇ               ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            ‚îÇ ?                      ‚îÇ             \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_48 (\u001b[38;5;33mDense\u001b[0m)                ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_49 (\u001b[38;5;33mDense\u001b[0m)                ‚îÇ ?                      ‚îÇ   \u001b[38;5;34m0\u001b[0m (unbuilt) ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 537ms/step - accuracy: 0.9519 - loss: 0.1546 - val_accuracy: 0.9852 - val_loss: 0.0516\n",
      "Epoch 2/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 523ms/step - accuracy: 0.9912 - loss: 0.0312 - val_accuracy: 0.9918 - val_loss: 0.0266\n",
      "Epoch 3/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 528ms/step - accuracy: 0.9946 - loss: 0.0177 - val_accuracy: 0.9929 - val_loss: 0.0202\n",
      "Epoch 4/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 523ms/step - accuracy: 0.9958 - loss: 0.0122 - val_accuracy: 0.9937 - val_loss: 0.0172\n",
      "Epoch 5/5\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 520ms/step - accuracy: 0.9961 - loss: 0.0096 - val_accuracy: 0.9944 - val_loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# BUILD MODEL\n",
    "# =====================================\n",
    "model = Sequential([\n",
    "    Embedding(MAX_WORDS, 128, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    Bidirectional(LSTM(32)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# TRAIN\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "model.save(\"judol_detector_lstm.h5\")\n",
    "\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c67685dd-2144-4b1e-be21-9515eb0db29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Label: 0 Prob: 0.0020317535381764174\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN = 100\n",
    "\n",
    "# load\n",
    "model = tf.keras.models.load_model(\"judol_detector_lstm.h5\")\n",
    "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "def normalize(text):\n",
    "    # brand ke <BRAND>\n",
    "    text = brand_regex.sub(\" <BRAND> \", text)\n",
    "    return text\n",
    "\n",
    "def predict(text):\n",
    "    text = normalize(text)\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    pad = pad_sequences(seq, maxlen=MAX_LEN)\n",
    "    prob = float(model.predict(pad)[0][0])\n",
    "    label = 1 if prob >= 0.5 else 0\n",
    "    return label, prob\n",
    "\n",
    "# contoh\n",
    "test = \"promo maxwin olympus 100% hari ini\"\n",
    "label, prob = predict(test)\n",
    "print(\"Label:\", label, \"Prob:\", prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc8464-583e-4820-90e7-5ca5262e2cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
